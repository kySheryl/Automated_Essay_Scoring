{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"temp","provenance":[{"file_id":"1JfcYTHoUIGcp01p0ZPHLCvoqGEN2xKLL","timestamp":1635047906593},{"file_id":"1cwSlzkyV_LX7smU4QNn10m7xbzm6K2p4","timestamp":1634859392015},{"file_id":"/v2/external/notebooks/intro.ipynb","timestamp":1634781473821}],"collapsed_sections":["sQcSDXc2o9il","u-D3hsOSjPoR","U_jFIUisjVz6"],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5fCEDCU_qrC0"},"source":["<p><img alt=\"Colaboratory logo\" height=\"45px\" src=\"https://avatars.githubusercontent.com/u/68501989?s=200&v=4\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n","\n","# Automated Essay Grading: Memory Networks\n","An implementation of paper \"[A Memory-Augmented Neural Model for Automated Grading](https://dl.acm.org/ft_gateway.cfm?id=3053982&ftid=1865482&dwn=1&CFID=92011169&CFTOKEN=5e86404040ee95d2-C94D3628-D29D-B939-636CA507CEAA3A7B)\" in PyTorch.\n","\n","Reference: https://github.com/yxwangnju/Memory-Networks-Automated-Essay-Grading/blob/master/train.py\n","\n","## Requirements\n","- PyTorch 1.2.0\n","- scikit-learn\n","- six\n","- python3\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oZHb0_4KeKnK","executionInfo":{"status":"ok","timestamp":1638334616466,"user_tz":300,"elapsed":20663,"user":{"displayName":"Yue Kuang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1vp258jkiryvm9TTrfO-iEBUB6bO5qRD-zHYJ=s64","userId":"13068211838488884124"}},"outputId":"1f948ca9-4a19-41ca-feca-2d1b8f0b62f8"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KF-9LvUJdVog","executionInfo":{"status":"ok","timestamp":1638334618159,"user_tz":300,"elapsed":1697,"user":{"displayName":"Yue Kuang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1vp258jkiryvm9TTrfO-iEBUB6bO5qRD-zHYJ=s64","userId":"13068211838488884124"}},"outputId":"02377088-4569-4646-c7ee-d39c1da8ba23"},"source":["%cd /content/drive/MyDrive/6998Cloud/6998reading/Project"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/6998Cloud/6998reading/Project\n"]}]},{"cell_type":"code","metadata":{"id":"wCuTi7sg6fsT"},"source":["SET_ID = 3"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x57VLFtlohJQ"},"source":["## Dataset\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IGIVaj63gRdn","executionInfo":{"status":"ok","timestamp":1638334974461,"user_tz":300,"elapsed":306,"user":{"displayName":"Yue Kuang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1vp258jkiryvm9TTrfO-iEBUB6bO5qRD-zHYJ=s64","userId":"13068211838488884124"}},"outputId":"bfee0070-a189-4abb-e114-7b7e39e2f5b0"},"source":["import pandas as pd\n","df = pd.read_csv('essays.csv')\n","df.head(5)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>prompt</th>\n","      <th>userid</th>\n","      <th>response</th>\n","      <th>points</th>\n","      <th>max_points</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>304240</td>\n","      <td>&lt;p&gt;Do you agree or disagree with the following...</td>\n","      <td>14838202928307690</td>\n","      <td>&lt;p&gt;Most of the student prefer to study withe c...</td>\n","      <td>6.75</td>\n","      <td>12.5</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>304240</td>\n","      <td>&lt;p&gt;Do you agree or disagree with the following...</td>\n","      <td>15138142465946030</td>\n","      <td>&lt;p&gt;&amp;nbsp;These days, there are various ways an...</td>\n","      <td>8.00</td>\n","      <td>12.5</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>304240</td>\n","      <td>&lt;p&gt;Do you agree or disagree with the following...</td>\n","      <td>15245269872001620</td>\n","      <td>&lt;p&gt;I totally disagree coaches are better than ...</td>\n","      <td>7.50</td>\n","      <td>12.5</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>304240</td>\n","      <td>&lt;p&gt;Do you agree or disagree with the following...</td>\n","      <td>15257228153021210</td>\n","      <td>&lt;p&gt;Trainers of sports activities&amp;nbsp;&lt;/p&gt;\\n\\n...</td>\n","      <td>5.00</td>\n","      <td>12.5</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>304240</td>\n","      <td>&lt;p&gt;Do you agree or disagree with the following...</td>\n","      <td>1475518065937950</td>\n","      <td>&lt;p style=\"text-align: center;\"&gt;&lt;span style=\"fo...</td>\n","      <td>6.25</td>\n","      <td>12.5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       id  ... max_points\n","0  304240  ...       12.5\n","1  304240  ...       12.5\n","2  304240  ...       12.5\n","3  304240  ...       12.5\n","4  304240  ...       12.5\n","\n","[5 rows x 6 columns]"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"twKo87akP4wa","executionInfo":{"status":"ok","timestamp":1638334974461,"user_tz":300,"elapsed":22,"user":{"displayName":"Yue Kuang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1vp258jkiryvm9TTrfO-iEBUB6bO5qRD-zHYJ=s64","userId":"13068211838488884124"}},"outputId":"5a5cfd50-a738-461f-83df-b0067b1feee1"},"source":["df.prompt.unique()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['<p>Do you agree or disagree with the following statement? Coaches are the best teachers. Use specific reasons and examples to support your answer.</p>\\n',\n","       \"<p>Do you agree or disagree with the following statement? Universities should give the same amount of money to their students' sports activities as they give to their university libraries. Use specific reasons and examples to support your opinion.</p>\\n\",\n","       '<p>Do you agree or disagree with the following statement? Television, newspapers, magazines, and other media pay too much attention to the personal lives of famous people such as public figures and celebrities. Use specific reasons and details to explain your opinion.</p>\\n',\n","       '<p>People attend school for many different reasons (for example, expanded knowledge, societal awareness, and enhanced interpersonal relationships). Why do you think people decide to go to school? Use specific reasons and examples to support your answer.</p>\\n',\n","       '<p>Write about one of your favorite family traditions.\\xa0Use specific reasons and examples to support your answer.</p>\\n',\n","       '<p><span style=\"font-family:verdana,arial,helvetica,sans-serif; font-size:12.8px\">There are many skills that will help people be successful today. \\xa0What is a very important skill a person should learn in order to be successful in the world today? Choose one skill and use specific reasons and examples to support your choice.\\xa0</span></p>\\n',\n","       '<p><span style=\"font-family:verdana,arial,helvetica,sans-serif; font-size:12.8px\">There are many types of entertainment today. \\xa0Movies and telivision are very popular. \\xa0How do movies or television influence people’s behavior? Use reasons and specific examples to support your answer.</span></p>\\n',\n","       '<p><span style=\"font-family:verdana,arial,helvetica,sans-serif; font-size:12.8px\">Do you agree or disagree with the following statement? Important decisions should never be made alone. Use specific reasons and examples to support your answer.</span></p>\\n',\n","       '<p>Do you agree or disagree with the following statement? Being self confident is the most important character trait that you can have.\\xa0 Use specific reasons for your answer.</p>\\n',\n","       '<p><span style=\"font-family:verdana,arial,helvetica,sans-serif; font-size:12.8px\">\\xa0Some say that physical exercise should be a required part of every school day. Others\\xa0believe that students should spend the whole school day on academic studies. Which opinion do you agree with? Use specific reasons and details to support your answer.</span></p>\\n',\n","       '<p><span style=\"font-family:verdana,arial,helvetica,sans-serif; font-size:12.8px\">What do you consider to be the most important subject in school? Why is this subject more important to you than any other subject? Use specific reasons and examples to support your opinion.</span></p>\\n',\n","       '<p><span style=\"font-family:verdana,arial,helvetica,sans-serif; font-size:12.8px\">Some people prefer to plan activities for their free time very carefully. Others choose not to make any plans at all for their free time. Think about the benefits of planning free-time activities vs\\xa0the benefits of not making plans. Which do you prefer\\xa0planning or not planning your leisure time? Use specific reasons and examples to explain your choice.</span></p>\\n',\n","       '<p>Write about your favorite vacation spot.\\xa0 Describe it and write about why you like it so much.</p>\\n\\n<p>\\xa0</p>\\n',\n","       \"<p>Write about the role of technology in today's world.\\xa0 Compare and contrast the advantages and the disadvantages.</p>\\n\"],\n","      dtype=object)"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"u3f2I_rI9JIg"},"source":["# drop all rows with any NaN and NaT values\n","df = df.dropna()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H_E20XufEbCi"},"source":["def my_clean(input):\n","  result = input\n","  while '<style' in result:\n","    result = result[:result.find('<style')] + result[result.find('</style>') + 8:]\n","  return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cXXQqdPYGRaX","executionInfo":{"status":"ok","timestamp":1638334974463,"user_tz":300,"elapsed":22,"user":{"displayName":"Yue Kuang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1vp258jkiryvm9TTrfO-iEBUB6bO5qRD-zHYJ=s64","userId":"13068211838488884124"}},"outputId":"00bccf12-3307-4c5c-8443-7783d2679440"},"source":["my_clean('<p>I like my family every summer go the<style type=\"text/css\">p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px STHeiti; color: #313131; -webkit-text-stroke: #313131}span.s1 {font-kerning: none}</style><span class=\"s1\">different&nbsp;</span><style type=\"text/css\">p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px STHeiti; color: #313131; -webkit-text-stroke: #313131}span.s1 {font-kerning: none}</style><span class=\"s1\">country travel, because this can make me know everycountry is different and has different life, so every summer I went to many country, and I have many friend in the different county. my family teach me the world is big and has many different.</span></p><p><span class=\"s1\">so this is my favorite family traditions so anywere travel.</span></p>')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<p>I like my family every summer go the<span class=\"s1\">different&nbsp;</span><span class=\"s1\">country travel, because this can make me know everycountry is different and has different life, so every summer I went to many country, and I have many friend in the different county. my family teach me the world is big and has many different.</span></p><p><span class=\"s1\">so this is my favorite family traditions so anywere travel.</span></p>'"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yEbfJF2B4wZe","executionInfo":{"status":"ok","timestamp":1638334978347,"user_tz":300,"elapsed":3905,"user":{"displayName":"Yue Kuang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1vp258jkiryvm9TTrfO-iEBUB6bO5qRD-zHYJ=s64","userId":"13068211838488884124"}},"outputId":"e42f7055-66b4-4dd4-c294-e3d2fe8d94c8"},"source":[" !pip install beautifulsoup4"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kb2T2pG34zC6","executionInfo":{"status":"ok","timestamp":1638334978348,"user_tz":300,"elapsed":7,"user":{"displayName":"Yue Kuang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1vp258jkiryvm9TTrfO-iEBUB6bO5qRD-zHYJ=s64","userId":"13068211838488884124"}},"outputId":"69be3db0-89b3-486f-ae02-ff626ba3e4d2"},"source":["from bs4 import BeautifulSoup\n","soup = BeautifulSoup(\"<p>Write about the role of technology in today's world.\\xa0 Compare and contrast the advantages and the disadvantages.</p>\\n\")\n","print(soup.text)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Write about the role of technology in today's world.  Compare and contrast the advantages and the disadvantages.\n","\n"]}]},{"cell_type":"code","metadata":{"id":"Shble-ti5V_Y"},"source":["response = [x for x in df['response']]\n","for i in range(len(response)):\n","  try:\n","    result = my_clean(response[i])\n","    response[i] = BeautifulSoup(result).text\n","  except AttributeError:\n","        pass\n","df['response'] = response"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vCiml9C3gXd2","executionInfo":{"status":"ok","timestamp":1638334979045,"user_tz":300,"elapsed":60,"user":{"displayName":"Yue Kuang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1vp258jkiryvm9TTrfO-iEBUB6bO5qRD-zHYJ=s64","userId":"13068211838488884124"}},"outputId":"a183c56e-2cbe-4f52-b35b-cdd921ba86f3"},"source":["prompt2setid = {}\n","i=1\n","for p in df.prompt.unique():\n","  prompt2setid[p] = i\n","  i+=1\n","print(prompt2setid)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'<p>Do you agree or disagree with the following statement? Coaches are the best teachers. Use specific reasons and examples to support your answer.</p>\\n': 1, \"<p>Do you agree or disagree with the following statement? Universities should give the same amount of money to their students' sports activities as they give to their university libraries. Use specific reasons and examples to support your opinion.</p>\\n\": 2, '<p>Do you agree or disagree with the following statement? Television, newspapers, magazines, and other media pay too much attention to the personal lives of famous people such as public figures and celebrities. Use specific reasons and details to explain your opinion.</p>\\n': 3, '<p>People attend school for many different reasons (for example, expanded knowledge, societal awareness, and enhanced interpersonal relationships). Why do you think people decide to go to school? Use specific reasons and examples to support your answer.</p>\\n': 4, '<p>Write about one of your favorite family traditions.\\xa0Use specific reasons and examples to support your answer.</p>\\n': 5, '<p><span style=\"font-family:verdana,arial,helvetica,sans-serif; font-size:12.8px\">There are many skills that will help people be successful today. \\xa0What is a very important skill a person should learn in order to be successful in the world today? Choose one skill and use specific reasons and examples to support your choice.\\xa0</span></p>\\n': 6, '<p><span style=\"font-family:verdana,arial,helvetica,sans-serif; font-size:12.8px\">There are many types of entertainment today. \\xa0Movies and telivision are very popular. \\xa0How do movies or television influence people’s behavior? Use reasons and specific examples to support your answer.</span></p>\\n': 7, '<p><span style=\"font-family:verdana,arial,helvetica,sans-serif; font-size:12.8px\">Do you agree or disagree with the following statement? Important decisions should never be made alone. Use specific reasons and examples to support your answer.</span></p>\\n': 8, '<p>Do you agree or disagree with the following statement? Being self confident is the most important character trait that you can have.\\xa0 Use specific reasons for your answer.</p>\\n': 9, '<p><span style=\"font-family:verdana,arial,helvetica,sans-serif; font-size:12.8px\">\\xa0Some say that physical exercise should be a required part of every school day. Others\\xa0believe that students should spend the whole school day on academic studies. Which opinion do you agree with? Use specific reasons and details to support your answer.</span></p>\\n': 10, '<p><span style=\"font-family:verdana,arial,helvetica,sans-serif; font-size:12.8px\">What do you consider to be the most important subject in school? Why is this subject more important to you than any other subject? Use specific reasons and examples to support your opinion.</span></p>\\n': 11, '<p><span style=\"font-family:verdana,arial,helvetica,sans-serif; font-size:12.8px\">Some people prefer to plan activities for their free time very carefully. Others choose not to make any plans at all for their free time. Think about the benefits of planning free-time activities vs\\xa0the benefits of not making plans. Which do you prefer\\xa0planning or not planning your leisure time? Use specific reasons and examples to explain your choice.</span></p>\\n': 12, '<p>Write about your favorite vacation spot.\\xa0 Describe it and write about why you like it so much.</p>\\n\\n<p>\\xa0</p>\\n': 13, \"<p>Write about the role of technology in today's world.\\xa0 Compare and contrast the advantages and the disadvantages.</p>\\n\": 14}\n"]}]},{"cell_type":"code","metadata":{"id":"55la4Zh6glda"},"source":["df['set_id'] = [prompt2setid[x] for x in df['prompt']]\n","points = [int(10*float(str(x)[:3])) for x in df['points']]\n","df['points'] =   points\n","df['essay_id'] = [i for i in range(len(df))]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eNsi8VgEXO8L","executionInfo":{"status":"ok","timestamp":1638334979049,"user_tz":300,"elapsed":56,"user":{"displayName":"Yue Kuang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1vp258jkiryvm9TTrfO-iEBUB6bO5qRD-zHYJ=s64","userId":"13068211838488884124"}},"outputId":"35385f49-2991-4496-da61-d240822d2678"},"source":["df.head(5)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>prompt</th>\n","      <th>userid</th>\n","      <th>response</th>\n","      <th>points</th>\n","      <th>max_points</th>\n","      <th>set_id</th>\n","      <th>essay_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>304240</td>\n","      <td>&lt;p&gt;Do you agree or disagree with the following...</td>\n","      <td>14838202928307690</td>\n","      <td>Most of the student prefer to study withe coac...</td>\n","      <td>67</td>\n","      <td>12.5</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>304240</td>\n","      <td>&lt;p&gt;Do you agree or disagree with the following...</td>\n","      <td>15138142465946030</td>\n","      <td>These days, there are various ways and method...</td>\n","      <td>80</td>\n","      <td>12.5</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>304240</td>\n","      <td>&lt;p&gt;Do you agree or disagree with the following...</td>\n","      <td>15245269872001620</td>\n","      <td>I totally disagree coaches are better than the...</td>\n","      <td>75</td>\n","      <td>12.5</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>304240</td>\n","      <td>&lt;p&gt;Do you agree or disagree with the following...</td>\n","      <td>15257228153021210</td>\n","      <td>Trainers of sports activities \\n    Trainers o...</td>\n","      <td>50</td>\n","      <td>12.5</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>304240</td>\n","      <td>&lt;p&gt;Do you agree or disagree with the following...</td>\n","      <td>1475518065937950</td>\n","      <td>Coaches are the best teachers\\nI AGREE WITH TH...</td>\n","      <td>62</td>\n","      <td>12.5</td>\n","      <td>1</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       id                                             prompt  ...  set_id essay_id\n","0  304240  <p>Do you agree or disagree with the following...  ...       1        0\n","1  304240  <p>Do you agree or disagree with the following...  ...       1        1\n","2  304240  <p>Do you agree or disagree with the following...  ...       1        2\n","3  304240  <p>Do you agree or disagree with the following...  ...       1        3\n","4  304240  <p>Do you agree or disagree with the following...  ...       1        4\n","\n","[5 rows x 8 columns]"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","metadata":{"id":"2H-7uf-CgvW4"},"source":["from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LjF1JJSLhI-D","executionInfo":{"status":"ok","timestamp":1638334979054,"user_tz":300,"elapsed":58,"user":{"displayName":"Yue Kuang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1vp258jkiryvm9TTrfO-iEBUB6bO5qRD-zHYJ=s64","userId":"13068211838488884124"}},"outputId":"33fce12c-4be1-41b2-f343-232d11170426"},"source":["df = df[df['set_id']==SET_ID]\n","len(df)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["155"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","metadata":{"id":"cfx2YKh52P82","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638334979056,"user_tz":300,"elapsed":55,"user":{"displayName":"Yue Kuang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1vp258jkiryvm9TTrfO-iEBUB6bO5qRD-zHYJ=s64","userId":"13068211838488884124"}},"outputId":"23056cbb-b551-4914-c4b4-2a709f59f2af"},"source":["df['range'] = [int((point-5)/10) for point in df['points']]"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \"\"\"Entry point for launching an IPython kernel.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zY6L9bDrhBO3","executionInfo":{"status":"ok","timestamp":1638334979060,"user_tz":300,"elapsed":56,"user":{"displayName":"Yue Kuang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1vp258jkiryvm9TTrfO-iEBUB6bO5qRD-zHYJ=s64","userId":"13068211838488884124"}},"outputId":"57446785-2704-498f-cee3-6e63073fa039"},"source":["train, test = train_test_split(df, test_size=0.1, stratify = df['range'], random_state = 20)\n","train, dev = train_test_split(train, test_size=0.2, stratify = train['range'], random_state = 20)\n","print(len(train), len(dev), len(test))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["111 28 16\n"]}]},{"cell_type":"code","metadata":{"id":"HETOLnQCmxtK"},"source":["train = train.sort_values(['points'])\n","dev = dev.sort_values(['points'])\n","test = test.sort_values(['points'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C0rb0Xn7YCAv"},"source":["from collections import Counter"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"24qF-RITX_t4","executionInfo":{"status":"ok","timestamp":1638334979064,"user_tz":300,"elapsed":54,"user":{"displayName":"Yue Kuang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1vp258jkiryvm9TTrfO-iEBUB6bO5qRD-zHYJ=s64","userId":"13068211838488884124"}},"outputId":"cf92541e-6065-4f34-d864-8919dd109526"},"source":["Counter(train[train[\"set_id\"]==3].points)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Counter({10: 2,\n","         15: 2,\n","         30: 2,\n","         35: 1,\n","         37: 1,\n","         45: 2,\n","         50: 7,\n","         56: 2,\n","         59: 1,\n","         60: 5,\n","         62: 11,\n","         65: 2,\n","         70: 10,\n","         71: 3,\n","         72: 5,\n","         73: 2,\n","         75: 14,\n","         77: 2,\n","         78: 1,\n","         80: 6,\n","         81: 3,\n","         84: 1,\n","         85: 1,\n","         87: 10,\n","         93: 6,\n","         96: 4,\n","         100: 3,\n","         120: 2})"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y9gVwBH5p0id","executionInfo":{"status":"ok","timestamp":1638334979065,"user_tz":300,"elapsed":53,"user":{"displayName":"Yue Kuang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1vp258jkiryvm9TTrfO-iEBUB6bO5qRD-zHYJ=s64","userId":"13068211838488884124"}},"outputId":"4e53300a-e16b-4074-af0a-e18fbd84df92"},"source":["df.head(5)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>prompt</th>\n","      <th>userid</th>\n","      <th>response</th>\n","      <th>points</th>\n","      <th>max_points</th>\n","      <th>set_id</th>\n","      <th>essay_id</th>\n","      <th>range</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>331</th>\n","      <td>304242</td>\n","      <td>&lt;p&gt;Do you agree or disagree with the following...</td>\n","      <td>15056295592387190</td>\n","      <td>I think the media pay to much attention to he ...</td>\n","      <td>65</td>\n","      <td>12.5</td>\n","      <td>3</td>\n","      <td>329</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>332</th>\n","      <td>304242</td>\n","      <td>&lt;p&gt;Do you agree or disagree with the following...</td>\n","      <td>14722636982161030</td>\n","      <td>I agree that celebrities appear more often tha...</td>\n","      <td>100</td>\n","      <td>12.5</td>\n","      <td>3</td>\n","      <td>330</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>333</th>\n","      <td>304242</td>\n","      <td>&lt;p&gt;Do you agree or disagree with the following...</td>\n","      <td>14725828860074590</td>\n","      <td>The medias are the most important key for famo...</td>\n","      <td>75</td>\n","      <td>12.5</td>\n","      <td>3</td>\n","      <td>331</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>334</th>\n","      <td>304242</td>\n","      <td>&lt;p&gt;Do you agree or disagree with the following...</td>\n","      <td>1475526072730930</td>\n","      <td>i agree with it cause all this media make our ...</td>\n","      <td>15</td>\n","      <td>12.5</td>\n","      <td>3</td>\n","      <td>332</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>335</th>\n","      <td>304242</td>\n","      <td>&lt;p&gt;Do you agree or disagree with the following...</td>\n","      <td>14793723147003730</td>\n","      <td>xxx\\n</td>\n","      <td>100</td>\n","      <td>12.5</td>\n","      <td>3</td>\n","      <td>333</td>\n","      <td>9</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         id                                             prompt  ...  essay_id range\n","331  304242  <p>Do you agree or disagree with the following...  ...       329     6\n","332  304242  <p>Do you agree or disagree with the following...  ...       330     9\n","333  304242  <p>Do you agree or disagree with the following...  ...       331     7\n","334  304242  <p>Do you agree or disagree with the following...  ...       332     1\n","335  304242  <p>Do you agree or disagree with the following...  ...       333     9\n","\n","[5 rows x 9 columns]"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7uP2THEk3QPg","executionInfo":{"status":"ok","timestamp":1638334979066,"user_tz":300,"elapsed":51,"user":{"displayName":"Yue Kuang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1vp258jkiryvm9TTrfO-iEBUB6bO5qRD-zHYJ=s64","userId":"13068211838488884124"}},"outputId":"274c3c83-5b27-4021-c44f-0a595177fe74"},"source":["train['range']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["357     0\n","433     0\n","410     1\n","334     1\n","422     2\n","       ..\n","332     9\n","335     9\n","411     9\n","336    11\n","412    11\n","Name: range, Length: 111, dtype: int64"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BAn9UjLX3Xum","executionInfo":{"status":"ok","timestamp":1638334979067,"user_tz":300,"elapsed":51,"user":{"displayName":"Yue Kuang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1vp258jkiryvm9TTrfO-iEBUB6bO5qRD-zHYJ=s64","userId":"13068211838488884124"}},"outputId":"904d6772-a9fe-4836-d479-a7e36b763d4e"},"source":["dev['range']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["420    3\n","394    4\n","363    4\n","353    4\n","359    5\n","462    5\n","368    5\n","466    5\n","388    5\n","407    6\n","342    6\n","480    6\n","354    6\n","337    6\n","449    6\n","338    7\n","371    7\n","447    7\n","343    7\n","375    7\n","348    7\n","376    7\n","434    8\n","460    8\n","339    8\n","391    8\n","461    9\n","385    9\n","Name: range, dtype: int64"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qGBS36x-3ZSF","executionInfo":{"status":"ok","timestamp":1638334979647,"user_tz":300,"elapsed":629,"user":{"displayName":"Yue Kuang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1vp258jkiryvm9TTrfO-iEBUB6bO5qRD-zHYJ=s64","userId":"13068211838488884124"}},"outputId":"8fe43a2a-5b1d-4e5b-f6d9-ce5525b9b8d8"},"source":["test['range']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["366    3\n","402    4\n","484    5\n","383    5\n","444    5\n","474    6\n","419    6\n","400    6\n","401    7\n","378    7\n","372    7\n","380    7\n","403    8\n","467    8\n","428    8\n","408    9\n","Name: range, dtype: int64"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","metadata":{"id":"g5ReAaHchb53"},"source":["train.to_csv('train1.csv', index=False)\n","dev.to_csv('dev1.csv', index=False)\n","test.to_csv('test1.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sQcSDXc2o9il"},"source":["## Download Glove weights\n","Pre-trained word embeddings are used in this model. You can download `glove_42B_300d` from [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_VChwFRMpJlT","executionInfo":{"elapsed":5150,"status":"ok","timestamp":1635781176932,"user":{"displayName":"Ruobing Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10353211177617186584"},"user_tz":240},"outputId":"62f6a6eb-6bed-458c-db46-4f41df8ddf1d"},"source":["!gdown --id 1GLQFwedWiB4B7upZFbeaqyOiY-JH3bJF -O glove.42B.300d.zip\n"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1GLQFwedWiB4B7upZFbeaqyOiY-JH3bJF\n","To: /content/drive/.shortcut-targets-by-id/1bz21KaeL-z2XfXZ9uJmI3opFGH5KcABL/6998reading/Project/glove.42B.300d.zip\n"," 13% 240M/1.88G [00:02<00:14, 115MB/s]Traceback (most recent call last):\n","  File \"/usr/local/bin/gdown\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python2.7/dist-packages/gdown/cli.py\", line 61, in main\n","    quiet=args.quiet,\n","  File \"/usr/local/lib/python2.7/dist-packages/gdown/download.py\", line 102, in download\n","    f.write(chunk)\n","KeyboardInterrupt\n"," 13% 242M/1.88G [00:02<00:15, 107MB/s]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jY1xV1PThrrj","executionInfo":{"elapsed":5061,"status":"ok","timestamp":1635781181985,"user":{"displayName":"Ruobing Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10353211177617186584"},"user_tz":240},"outputId":"c831504c-ba6f-4292-945a-aa588425264f"},"source":["!mkdir glove\n","!mv glove.42B.300d.zip ./glove\n","!unzip ./glove/glove.42B.300d.zip -d ./glove"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["mkdir: cannot create directory ‘glove’: File exists\n","mv: cannot stat 'glove.42B.300d.zip': No such file or directory\n","Archive:  ./glove/glove.42B.300d.zip\n","replace ./glove/glove.42B.300d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q-HJJY1BYCi8","executionInfo":{"elapsed":2708,"status":"ok","timestamp":1635781184686,"user":{"displayName":"Ruobing Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10353211177617186584"},"user_tz":240},"outputId":"6cc4d017-ba5b-4df5-a616-ca1b74d40525"},"source":["!gdown --id 19PLbIkGZOiRE7581QDJOlgYdbrbr5qJm -O glove.6B.zip"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=19PLbIkGZOiRE7581QDJOlgYdbrbr5qJm\n","To: /content/drive/.shortcut-targets-by-id/1bz21KaeL-z2XfXZ9uJmI3opFGH5KcABL/6998reading/Project/glove.6B.zip\n"," 17% 147M/862M [00:01<00:07, 97.7MB/s]Traceback (most recent call last):\n","  File \"/usr/local/bin/gdown\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python2.7/dist-packages/gdown/cli.py\", line 61, in main\n","    quiet=args.quiet,\n","  File \"/usr/local/lib/python2.7/dist-packages/gdown/download.py\", line 102, in download\n","    f.write(chunk)\n","KeyboardInterrupt\n"," 18% 157M/862M [00:01<00:07, 97.2MB/s]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5-FhMEweYYhi","executionInfo":{"elapsed":1943,"status":"ok","timestamp":1635781186619,"user":{"displayName":"Ruobing Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10353211177617186584"},"user_tz":240},"outputId":"d982e119-57a0-43bb-c7dd-dbaa600da506"},"source":["!mv glove.6B.zip ./glove\n","!unzip ./glove/glove.6B.zip -d ./glove"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["mv: cannot stat 'glove.6B.zip': No such file or directory\n","Archive:  ./glove/glove.6B.zip\n","replace ./glove/glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TuiUhfyeiBK7","executionInfo":{"elapsed":596,"status":"ok","timestamp":1635781187208,"user":{"displayName":"Ruobing Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10353211177617186584"},"user_tz":240},"outputId":"16a2e1ee-a28b-4ff7-eef0-fd309649f093"},"source":["!ls ./glove/"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["glove.42B.300d.txt  glove.6B.100d.txt  glove.6B.300d.txt  glove.6B.zip\n","glove.42B.300d.zip  glove.6B.200d.txt  glove.6B.50d.txt\n"]}]},{"cell_type":"markdown","metadata":{"id":"rbDKb2HMjHoB"},"source":["## Import snippets"]},{"cell_type":"markdown","metadata":{"id":"bEyk8cQmjNNg"},"source":["### data.py"]},{"cell_type":"code","metadata":{"id":"FjZ9V4e6iP_3"},"source":["import re\n","import pandas as pd\n","import sys\n","\n","def load_train_data(set_id: int):\n","    train_data = pd.read_csv(\"train1.csv\")\n","    all_data = train_data\n","    contents = all_data[all_data['set_id'] == set_id]['response'].values\n","    essay_ids = all_data[all_data['set_id'] == set_id]['essay_id'].values\n","    essay_scores = all_data[all_data['set_id'] == set_id]['points'].values\n","    essay_contents = []\n","    for index, content in enumerate(contents):\n","        content = clean_str(content)\n","        essay_contents.append(tokenize(content))\n","    return essay_contents, list(essay_scores), list(essay_ids)\n","\n","\n","def load_dev_data(set_id: int):\n","    dev_data = pd.read_csv('dev1.csv')\n","    all_data = dev_data\n","    contents = all_data[all_data['set_id'] == set_id]['response'].values\n","    essay_ids = all_data[all_data['set_id'] == set_id]['essay_id'].values\n","    essay_scores = all_data[all_data['set_id'] == set_id]['points'].values\n","    essay_contents = []\n","    for index, content in enumerate(contents):\n","        content = clean_str(content)\n","        essay_contents.append(tokenize(content))\n","    return essay_contents, list(essay_scores), list(essay_ids)\n","\n","\n","def load_test_data(set_id: int):\n","    test_data = pd.read_csv('test1.csv')\n","    contents = test_data[test_data['set_id'] == set_id]['response'].values\n","    essay_ids = test_data[test_data['set_id'] == set_id]['essay_id'].values\n","    # essay_scores = test_data[test_data['set_id'] == set_id]['points'].values\n","    essay_contents = []\n","    for index, content in enumerate(contents):\n","        content = clean_str(content)\n","        essay_contents.append(tokenize(content))\n","    return essay_contents, list(essay_ids)\n","\n","\n","def all_vocab(list1, list2, list3):\n","    w_set = set()\n","    for i in range(len(list1)):\n","        for j in list1[i]:\n","            if j not in w_set:\n","                w_set.add(j)\n","    for i in range(len(list2)):\n","        for j in list2[i]:\n","            if j not in w_set:\n","                w_set.add(j)\n","    for i in range(len(list3)):\n","        for j in list3[i]:\n","            if j not in w_set:\n","                w_set.add(j)\n","    return w_set\n","\n","\n","def tokenize(sent):\n","    \"\"\"\n","    Return the tokens of a sentence including punctuation.\n","    >> tokenize('Bob dropped the apple. Where is the apple?')\n","        ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n","    >> tokenize('I don't know')\n","        ['I', 'don', '\\'', 'know']\n","    \"\"\"\n","    return [x.strip() for x in re.split('(\\W+)', sent) if x.strip()]\n","\n","\n","def clean_str(string):\n","    \"\"\"\n","    Tokenization/string cleaning for all datasets except for SST.\n","    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n","    \"\"\"\n","    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n","    string = re.sub(r\"\\'s\", \" \\'s\", string)\n","    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n","    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n","    string = re.sub(r\"\\'re\", \" \\'re\", string)\n","    string = re.sub(r\"\\'d\", \" \\'d\", string)\n","    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n","    string = re.sub(r\",\", \" , \", string)\n","    string = re.sub(r\"!\", \" ! \", string)\n","    string = re.sub(r\"\\(\", \" ( \", string)\n","    string = re.sub(r\"\\)\", \" ) \", string)\n","    string = re.sub(r\"\\?\", \" ? \", string)\n","    string = re.sub(r\"\\s{2,}\", \" \", string)\n","\n","    return string.strip().lower()\n","\n","\n","def load_glove(w_vocab, token_num=6, dim=50):\n","    word2vec = []\n","    word_to_index = {}\n","    # first word is nil\n","    word2vec.append([0]*dim)\n","    count = 1\n","    with open(\"./glove/glove.\"+str(token_num)+\"B.\" + str(dim) + \"d.txt\", encoding='utf-8') as f:\n","        for line in f:\n","            l = line.split()\n","            word = l[0]\n","            if word in w_vocab:\n","                vector = list(map(float, l[1:]))\n","                word_to_index[word] = count\n","                word2vec.append(vector)\n","                count += 1\n","    print(\"==> glove is loaded\")\n","    print(f\"word2vec total size :{sys.getsizeof(word2vec)/1024} KB\")\n","    return word_to_index, word2vec\n","\n","\n","def vectorize_data(data, word_to_index, sentence_size):\n","    E = []\n","    for essay in data:\n","        ls = max(0, sentence_size - len(essay))\n","        wl = []\n","        count = 0\n","        for w in essay:\n","            count += 1\n","            if count > sentence_size:\n","                break\n","            if w in word_to_index:\n","                wl.append(word_to_index[w])\n","            else:\n","                wl.append(0)\n","        wl += [0]*ls\n","        E.append(wl)\n","    return E"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u-D3hsOSjPoR"},"source":["### metric.py"]},{"cell_type":"code","metadata":{"id":"vyKjb3AGjJzx"},"source":["\"\"\"\n","This module contains a bunch of evaluation metrics that can be used to\n","evaluate the performance of learners.\n",":author: Michael Heilman (mheilman@ets.org)\n",":author: Nitin Madnani (nmadnani@ets.org)\n",":author: Dan Blanchard (dblanchard@ets.org)\n",":organization: ETS\n","\"\"\"\n","\n","from __future__ import print_function, unicode_literals\n","\n","import logging\n","import math\n","import numpy as np\n","from scipy.stats import kendalltau, spearmanr, pearsonr\n","from six import string_types\n","from six.moves import xrange as range\n","from sklearn.metrics import confusion_matrix, f1_score, SCORERS\n","# from sklearn.metrics import mean_squared_error\n","\n","\n","# Constants\n","_CORRELATION_METRICS = frozenset(['kendall_tau', 'spearman', 'pearson'])\n","\n","\n","def kappa(y_true, y_pred, weights=None, allow_off_by_one=False):\n","    \"\"\"\n","    Calculates the kappa inter-rater agreement between two the gold standard\n","    and the predicted ratings. Potential values range from -1 (representing\n","    complete disagreement) to 1 (representing complete agreement).  A kappa\n","    value of 0 is expected if all agreement is due to chance.\n","    In the course of calculating kappa, all items in `y_true` and `y_pred` will\n","    first be converted to floats and then rounded to integers.\n","    It is assumed that y_true and y_pred contain the complete range of possible\n","    ratings.\n","    This function contains a combination of code from yorchopolis's kappa-stats\n","    and Ben Hamner's Metrics projects on Github.\n","    :param y_true: The true/actual/gold labels for the data.\n","    :type y_true: array-like of float\n","    :param y_pred: The predicted/observed labels for the data.\n","    :type y_pred: array-like of float\n","    :param weights: Specifies the weight matrix for the calculation.\n","                    Options are:\n","                        -  None = unweighted-kappa\n","                        -  'quadratic' = quadratic-weighted kappa\n","                        -  'linear' = linear-weighted kappa\n","                        -  two-dimensional numpy array = a custom matrix of\n","                           weights. Each weight corresponds to the\n","                           :math:`w_{ij}` values in the wikipedia description\n","                           of how to calculate weighted Cohen's kappa.\n","    :type weights: str or numpy array\n","    :param allow_off_by_one: If true, ratings that are off by one are counted as\n","                             equal, and all other differences are reduced by\n","                             one. For example, 1 and 2 will be considered to be\n","                             equal, whereas 1 and 3 will have a difference of 1\n","                             for when building the weights matrix.\n","    :type allow_off_by_one: bool\n","    \"\"\"\n","    logger = logging.getLogger(__name__)\n","\n","    # Ensure that the lists are both the same length\n","    assert(len(y_true) == len(y_pred))\n","    # This rather crazy looking typecast is intended to work as follows:\n","    # If an input is an int, the operations will have no effect.\n","    # If it is a float, it will be rounded and then converted to an int\n","    # because the ml_metrics package requires ints.\n","    # If it is a str like \"1\", then it will be converted to a (rounded) int.\n","    # If it is a str that can't be typecast, then the user is\n","    # given a hopefully useful error message.\n","    # Note: numpy and python 3.3 use bankers' rounding.\n","    try:\n","        y_true = [int(np.round(float(y))) for y in y_true]\n","        y_pred = [int(np.round(float(y))) for y in y_pred]\n","    except ValueError as e:\n","        logger.error(\"For kappa, the labels should be integers or strings \"\n","                     \"that can be converted to ints (E.g., '4.0' or '3').\")\n","        raise e\n","\n","    # Figure out normalized expected values\n","    min_rating = min(min(y_true), min(y_pred))\n","    max_rating = max(max(y_true), max(y_pred))\n","\n","    # shift the values so that the lowest value is 0\n","    # (to support scales that include negative values)\n","    y_true = [y - min_rating for y in y_true]\n","    y_pred = [y - min_rating for y in y_pred]\n","\n","    # Build the observed/confusion matrix\n","    num_ratings = max_rating - min_rating + 1\n","    observed = confusion_matrix(y_true, y_pred,\n","                                labels=list(range(num_ratings)))\n","    num_scored_items = float(len(y_true))\n","\n","    # Build weight array if weren't passed one\n","    if isinstance(weights, string_types):\n","        wt_scheme = weights\n","        weights = None\n","    else:\n","        wt_scheme = ''\n","    if weights is None:\n","        weights = np.empty((num_ratings, num_ratings))\n","        for i in range(num_ratings):\n","            for j in range(num_ratings):\n","                diff = abs(i - j)\n","                if allow_off_by_one and diff:\n","                    diff -= 1\n","                if wt_scheme == 'linear':\n","                    weights[i, j] = diff\n","                elif wt_scheme == 'quadratic':\n","                    weights[i, j] = diff ** 2\n","                elif not wt_scheme:  # unweighted\n","                    weights[i, j] = bool(diff)\n","                else:\n","                    raise ValueError('Invalid weight scheme specified for '\n","                                     'kappa: {}'.format(wt_scheme))\n","\n","    hist_true = np.bincount(y_true, minlength=num_ratings)\n","    hist_true = hist_true[: num_ratings] / num_scored_items\n","    hist_pred = np.bincount(y_pred, minlength=num_ratings)\n","    hist_pred = hist_pred[: num_ratings] / num_scored_items\n","    expected = np.outer(hist_true, hist_pred)\n","\n","    # Normalize observed array\n","    observed = observed / num_scored_items\n","\n","    # If all weights are zero, that means no disagreements matter.\n","    k = 1.0\n","    if np.count_nonzero(weights):\n","        k -= (sum(sum(weights * observed)) / sum(sum(weights * expected)))\n","\n","    return k\n","\n","\n","\n","def kendall_tau(y_true, y_pred):\n","    \"\"\"\n","    Calculate Kendall's tau between ``y_true`` and ``y_pred``.\n","    :param y_true: The true/actual/gold labels for the data.\n","    :type y_true: array-like of float\n","    :param y_pred: The predicted/observed labels for the data.\n","    :type y_pred: array-like of float\n","    :returns: Kendall's tau if well-defined, else 0\n","    \"\"\"\n","    ret_score = kendalltau(y_true, y_pred)[0]\n","    return ret_score if not np.isnan(ret_score) else 0.0\n","\n","\n","\n","def spearman(y_true, y_pred):\n","    \"\"\"\n","    Calculate Spearman's rank correlation coefficient between ``y_true`` and\n","    ``y_pred``.\n","    :param y_true: The true/actual/gold labels for the data.\n","    :type y_true: array-like of float\n","    :param y_pred: The predicted/observed labels for the data.\n","    :type y_pred: array-like of float\n","    :returns: Spearman's rank correlation coefficient if well-defined, else 0\n","    \"\"\"\n","    ret_score = spearmanr(y_true, y_pred)[0]\n","    return ret_score if not np.isnan(ret_score) else 0.0\n","\n","\n","\n","def pearson(y_true, y_pred):\n","    \"\"\"\n","    Calculate Pearson product-moment correlation coefficient between ``y_true``\n","    and ``y_pred``.\n","    :param y_true: The true/actual/gold labels for the data.\n","    :type y_true: array-like of float\n","    :param y_pred: The predicted/observed labels for the data.\n","    :type y_pred: array-like of float\n","    :returns: Pearson product-moment correlation coefficient if well-defined,\n","              else 0\n","    \"\"\"\n","    ret_score = pearsonr(y_true, y_pred)[0]\n","    return ret_score if not np.isnan(ret_score) else 0.0\n","\n","\n","\n","def f1_score_least_frequent(y_true, y_pred):\n","    \"\"\"\n","    Calculate the F1 score of the least frequent label/class in ``y_true`` for\n","    ``y_pred``.\n","    :param y_true: The true/actual/gold labels for the data.\n","    :type y_true: array-like of float\n","    :param y_pred: The predicted/observed labels for the data.\n","    :type y_pred: array-like of float\n","    :returns: F1 score of the least frequent label\n","    \"\"\"\n","    least_frequent = np.bincount(y_true).argmin()\n","    return f1_score(y_true, y_pred, average=None)[least_frequent]\n","\n","\n","\n","def use_score_func(func_name, y_true, y_pred):\n","    \"\"\"\n","    Call the scoring function in `sklearn.metrics.SCORERS` with the given name.\n","    This takes care of handling keyword arguments that were pre-specified when\n","    creating the scorer. This applies any sign-flipping that was specified by\n","    `make_scorer` when the scorer was created.\n","    \"\"\"\n","    scorer = SCORERS[func_name]\n","    return scorer._sign * scorer._score_func(y_true, y_pred, **scorer._kwargs)\n","\n","\n","def mean_square_error(y_true, y_pred):\n","    \"\"\"\n","    Calculate the mean square error between predictions and true scores\n","    :param y_true: true score list\n","    :param y_pred: predicted score list\n","    return mean_square_error value\n","    \"\"\"\n","    # return mean_squared_error(y_true, y_pred) # use sklean default function\n","    y_true = np.asarray(y_true)\n","    y_pred = np.asarray(y_pred)\n","    mse = ((y_true-y_pred)**2).mean(axis=0)\n","    return float(mse)\n","\n","\n","def root_mean_square_error(y_true, y_pred):\n","    \"\"\"\n","    Calculate the mean square error between predictions and true scores\n","    :param y_true: true score list\n","    :param y_pred: predicted score list\n","    return mean_square_error value\n","    \"\"\"\n","    # return mean_squared_error(y_true, y_pred) # use sklean default function\n","    y_true = np.asarray(y_true)\n","    y_pred = np.asarray(y_pred)\n","    mse = ((y_true-y_pred)**2).mean(axis=0)\n","    return float(math.sqrt(mse))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U_jFIUisjVz6"},"source":["### model.py"]},{"cell_type":"code","metadata":{"id":"TGQQzW7OjU9L"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","\n","class MANM(nn.Module):\n","    def __init__(self, word_to_vec, max_sent_size, memory_num, embedding_size, feature_size,\n","                 score_range, hops, l2_lambda, keep_prob, device):\n","        super(MANM, self).__init__()\n","        self.max_sent_size = max_sent_size\n","        self.memory_num = memory_num\n","        self.hops = hops\n","        self.l2_lambda = l2_lambda\n","        self.keep_prob = keep_prob\n","        self.score_range = score_range\n","        self.feature_size = feature_size\n","        self.embedding_size = embedding_size\n","        self.device = device\n","        self.word_to_vec = torch.nn.Embedding.from_pretrained(torch.from_numpy(word_to_vec), freeze=True)\n","        # [embedding_size, max_sent_size]\n","        self.pos_encoding = self.position_encoding(self.max_sent_size, self.embedding_size).requires_grad_(False).to(self.device)\n","\n","        # shape [k, d]\n","        self.A = torch.nn.Embedding(self.feature_size, self.embedding_size).to(self.device)\n","        self.B = torch.nn.Embedding(self.feature_size, self.embedding_size).to(self.device)\n","        self.C = torch.nn.Embedding(self.feature_size, self.embedding_size).to(self.device)\n","        torch.nn.init.xavier_uniform_(self.A.weight)\n","        torch.nn.init.xavier_uniform_(self.B.weight)\n","        torch.nn.init.xavier_uniform_(self.C.weight)\n","        # shape [k, k]\n","        Rlist = []\n","        for i in range(self.hops):\n","            R = torch.nn.Embedding(self.feature_size, self.feature_size).to(self.device)\n","            torch.nn.init.xavier_uniform_(R.weight)\n","            Rlist.append(R)\n","        self.R_list = torch.nn.ModuleList(Rlist)\n","        # shape [k, r]\n","        self.W = torch.nn.Embedding(self.feature_size, self.score_range).to(self.device)\n","        torch.nn.init.xavier_uniform_(self.W.weight)\n","        # bias in last layer\n","        self.b = torch.nn.Parameter(torch.randn([self.score_range]))\n","\n","    def forward(self, contents_idx:np.ndarray, memories_idx:np.ndarray, scores:np.ndarray):\n","        contents_idx = torch.from_numpy(contents_idx).to(self.device).requires_grad_(False)\n","        memories_idx = torch.from_numpy(memories_idx).to(self.device).requires_grad_(False)\n","        # [batch_size, max_sent_size, embedding_size]\n","        contents = self.word_to_vec(contents_idx)\n","        # [batch_size, memory_num, max_sent_size, embedding_size]\n","        memories = self.word_to_vec(memories_idx)\n","        # emb_contents [batch_size, d]    d=embedding_size\n","        # emb_memories [batch_size, memory_num, d]\n","        emb_contents, emb_memories = self.input_representation_layer(contents, memories)\n","        dropout = torch.nn.Dropout(p=1 - self.keep_prob)\n","        emb_contents = dropout(emb_contents).requires_grad_(False)\n","        # [batch_size, k] = [batch_size, d] x [d, k]\n","        u = torch.matmul(emb_contents, self.A.weight.transpose(0, 1))\n","\n","        for i in range(self.hops):\n","            prob_vectors, used_emb_memories = self.memory_addressing_layer(u, emb_memories)  # [batch_size, memory_num]\n","            u = self.memory_reading_layer(i, u, prob_vectors, used_emb_memories)  # [batch_size, k]\n","\n","        # [batch_size, memory_num]   distribution is softmax(logits)\n","        logits, distribution = self.output_layer(u)\n","        losser = torch.nn.CrossEntropyLoss()\n","        scores = torch.from_numpy(scores).requires_grad_(False).to(self.device)\n","        loss1 = torch.sum(losser(logits, scores))  # score: [batch_size]\n","        loss2 = (torch.sum(self.A.weight**2) + torch.sum(self.B.weight**2) +\n","                 torch.sum(self.C.weight**2) + torch.sum(self.W.weight**2) + torch.sum(self.b**2))/2\n","        for m in range(self.hops):\n","            loss2 += torch.sum(self.R_list[m].weight**2)/2\n","        loss = loss1+loss2*self.l2_lambda\n","        return loss\n","\n","    def position_encoding(self, sentence_size, embedding_size):\n","        encoding = np.ones((embedding_size, sentence_size), dtype=np.float32)\n","        ls = sentence_size + 1\n","        le = embedding_size + 1\n","        for i in range(1, le):\n","            for j in range(1, ls):\n","                encoding[i - 1, j - 1] = (i - (le - 1) / 2) * (j - (ls - 1) / 2)\n","        encoding = 1 + 4 * encoding / embedding_size / sentence_size\n","        pos_encoding = torch.from_numpy(encoding)\n","        return pos_encoding.transpose(0, 1)\n","\n","    def input_representation_layer(self, contents: torch.Tensor, memories: torch.Tensor):\n","        '''bow'''\n","        # contents [batch_size, max_sent_size, embedding_size]\n","        # memories [batch_size, memory_num, max_sent_size, embedding_size]\n","        # self.pos_encoding: [max_sent_size, embedding_size]\n","        emb_contents = torch.sum(contents*self.pos_encoding, dim=1).requires_grad_(False)  # *表示对位相乘\n","        # print(memories.shape,  self.pos_encoding.shape)\n","        emb_memories = torch.sum(memories*self.pos_encoding, dim=2).requires_grad_(False)  # *表示对位相乘\n","        return emb_contents, emb_memories\n","\n","    def memory_addressing_layer(self, u, emb_memories):\n","        dropout = torch.nn.Dropout(p=1-self.keep_prob)\n","        used_emb_memories = dropout(emb_memories).requires_grad_(False)\n","        # [batch_size, memory_num, k] = [batch_size, memory_num, d] x [d, k]\n","        trans_emb_memories = torch.matmul(used_emb_memories, self.B.weight.transpose(0,1))\n","        # dot product\n","        # [batch_size, memory_num, k] <- [batch_size, k]\n","        trans_emb_contents = u.unsqueeze(dim=1)\n","        # product [batch_size, memory_num]  *表示对位相乘\n","        product = torch.sum(trans_emb_contents*trans_emb_memories, dim=-1)  # 对最后一维进行sum\n","        # prob_vectors [batch_size, memory_num]\n","        prob_vectors = F.softmax(product, dim=-1)\n","        return prob_vectors, used_emb_memories\n","\n","    def memory_reading_layer(self, i, u, prob_vectors, used_emb_memories):\n","        # [batch_size, memory_num, 1]\n","        prob_vectors = torch.unsqueeze(prob_vectors, dim=2)\n","        # [batch_size * memory_num, d]\n","        memo_temp = used_emb_memories.view(-1, self.embedding_size)\n","        # print(\"used_emb_memories size: \", memo_temp.shape)\n","        # [d, batch_size * memory_num]\n","        memo_temp = memo_temp.transpose(0, 1)\n","        # [k, batch_size * memory_num]\n","        product = torch.matmul(self.C.weight, memo_temp)\n","        # print(product.shape)\n","        # [batch_size, memory_num, k]\n","        product = torch.reshape(product.transpose(0, 1), [-1, self.memory_num, self.feature_size])\n","        # product = torch.matmul(used_emb_memories, self.C.weight.transpose(0,1))\n","        # [batch_size, k]\n","        o = torch.sum(prob_vectors*product, dim=1)\n","        # [batch_size, k]\n","        u = F.relu(torch.matmul((o+u), self.R_list[i].weight))\n","        return u\n","\n","    def output_layer(self, u):\n","        # [batch_size, score_range]\n","        logits = torch.matmul(u, self.W.weight)+self.b\n","        distribution = F.softmax(logits, dim=1)\n","        return logits, distribution\n","\n","    def test(self, contents_idx, memories_idx):\n","        contents_idx = torch.from_numpy(contents_idx).to(self.device).requires_grad_(False)\n","        memories_idx = torch.from_numpy(memories_idx).to(self.device).requires_grad_(False)\n","        # [batch_size, max_sent_size, embedding_size]\n","        contents = self.word_to_vec(contents_idx)\n","        # [batch_size, memory_num, max_sent_size, embedding_size]\n","        memories = self.word_to_vec(memories_idx)\n","        # emb_contents [batch_size, d]    d=embedding_size\n","        # emb_memories [batch_size, memory_num, d]\n","        emb_contents, emb_memories = self.input_representation_layer(contents, memories)\n","        self.keep_prob = 1\n","        # [batch_size, k] = [batch_size, d] x [d, k]\n","        u = torch.matmul(emb_contents, self.A.weight.transpose(0, 1))\n","\n","        for i in range(self.hops):\n","            prob_vectors, used_emb_memories = self.memory_addressing_layer(u, emb_memories)  # [batch_size, memory_num]\n","            u = self.memory_reading_layer(i, u, prob_vectors, used_emb_memories)  # [batch_size, k]\n","\n","        # [batch_size, memory_num]\n","        logits, distribution = self.output_layer(u)\n","        # print(distribution)\n","        # [batch_size]\n","        pred_scores = torch.argmax(distribution, dim=1)\n","        return pred_scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LeCdwFloomO9","executionInfo":{"status":"ok","timestamp":1638335090294,"user_tz":300,"elapsed":5,"user":{"displayName":"Yue Kuang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1vp258jkiryvm9TTrfO-iEBUB6bO5qRD-zHYJ=s64","userId":"13068211838488884124"}},"outputId":"610efb21-bd0d-4673-bf8c-80e716a4ec37"},"source":["import torch\n","print(torch.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.10.0+cu111\n"]}]},{"cell_type":"markdown","metadata":{"id":"e8Z2bnxZjhe0"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"bPHVqGQZj4LP"},"source":["CONFIGS = {\"gpu_id\" : 0,\n","\"set_id\" : SET_ID ,#essay set id, 1 <: id <: 8\n","\"emb_size\" : 300 ,#Embedding size for sentences\n","\"token_num\" : 42 ,#The number of token in glove (6, 42\n","\"feature_size\" : 100 ,#Feature size\n","\"epochs\" : 200 ,#Number of epochs to train for\n","\"test_freq\" : 20 ,#Evaluate and print results every x epochs\n","\"hops\" : 1 ,#Number of hops in the Memory Network\n","\"lr\" : 0.0005,#Learning rate\n","\"batch_size\" : 16 ,#Batch size for training\n","\"l2_lambda\" : 0.1,#Lambda for l2 loss\n","\"num_samples\" : 1 ,#Number of samples selected as memories for\n","\"epsilon\" : 0.1 ,#Epsilon value for Adam Optimizer\n","\"max_grad_norm\" : 10.0 ,#Clip gradients to this norm\n","\"keep_prob\" : 0.9 ,#Keep probability for dropout\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ojgN9aE9XCy7"},"source":["params = [(1,0.0005, 0.1, 0.1, 10.0)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"huZjgKjAPTM4"},"source":["# params = [] # hops, lr, l2_lambda, epsilon, max_grad_norm\n","# for hops in [1,2]:\n","#   for lr in [0.0005, 0.001, 0.002]:\n","#     for l2_lambda in [0.1, 0.2]:\n","#       for epsilon in [0.1,0.01]:\n","#         for max_grad_norm in [2.0, 6.0, 10.0]:\n","#           params.append((hops, lr, l2_lambda, epsilon, max_grad_norm))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"shpl5pHOjaNi","executionInfo":{"status":"ok","timestamp":1638335090664,"user_tz":300,"elapsed":8,"user":{"displayName":"Yue Kuang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1vp258jkiryvm9TTrfO-iEBUB6bO5qRD-zHYJ=s64","userId":"13068211838488884124"}},"outputId":"8a92dd47-02fc-4f40-df00-30223ede0f34"},"source":["if torch.cuda.is_available():\n","  print(f\"Using GPU:{CONFIGS['gpu_id']}\")\n","  device = torch.device(\"cuda\")\n","  torch.cuda.set_device(CONFIGS['gpu_id'])\n","else:\n","  print(\"!!! Using CPU\")\n","  device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using GPU:0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EmQYz0avjqkY","executionInfo":{"status":"ok","timestamp":1638335090665,"user_tz":300,"elapsed":8,"user":{"displayName":"Yue Kuang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1vp258jkiryvm9TTrfO-iEBUB6bO5qRD-zHYJ=s64","userId":"13068211838488884124"}},"outputId":"a2291805-769e-4f6f-8ca4-573b1ca04ae6"},"source":["!mkdir ./logs"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘./logs’: File exists\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Aw9mdr8j1yX","executionInfo":{"status":"ok","timestamp":1638335090665,"user_tz":300,"elapsed":7,"user":{"displayName":"Yue Kuang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1vp258jkiryvm9TTrfO-iEBUB6bO5qRD-zHYJ=s64","userId":"13068211838488884124"}},"outputId":"409f3f1f-b4c7-47f2-dbd9-a7add9be7b22"},"source":["import numpy as np\n","# read training, dev and test data\n","train_essay_contents, train_essay_scores, train_essay_ids = load_train_data(CONFIGS['set_id'])\n","dev_essay_contents, dev_essay_scores, dev_essay_ids = load_dev_data(CONFIGS['set_id'])\n","test_essay_contents, test_essay_ids = load_test_data(CONFIGS['set_id'])\n","# min_score = min(train_essay_scores)\n","# max_score = max(train_essay_scores)\n","min_score = 5\n","max_score = 125\n","score_range = list(np.arange(5,126,1))\n","\n","# get the vocabulary of training, dev and test datasets.\n","all_vocab = all_vocab(train_essay_contents, dev_essay_contents, test_essay_contents)\n","print(f\"all_vocab len:{len(all_vocab)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["all_vocab len:2361\n"]}]},{"cell_type":"code","metadata":{"id":"OU25p-Ih5osc"},"source":["import json\n","with open('all_vocab.txt', 'w') as f:\n","  f.write(json.dumps(list(all_vocab)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IL70Z9J0o1EW","executionInfo":{"status":"ok","timestamp":1638335102038,"user_tz":300,"elapsed":11379,"user":{"displayName":"Yue Kuang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1vp258jkiryvm9TTrfO-iEBUB6bO5qRD-zHYJ=s64","userId":"13068211838488884124"}},"outputId":"47befd42-ca87-41c1-e5d3-f7ccce1d90fc"},"source":["# loading glove. Only select words which appear in vocabulary.\n","print(\"Loading Glove.....\")\n","# t1 = time.time()\n","word_to_index, word_to_vec = load_glove(w_vocab=all_vocab, token_num=CONFIGS['token_num'], dim=CONFIGS['emb_size'])\n","word_to_vec = np.array(word_to_vec, dtype=np.float32)\n","# t2 = time.time()\n","# print(f\"Finished loading Glove!, time cost = {(t2-t1):.4f}s\\n\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading Glove.....\n","==> glove is loaded\n","word2vec total size :18.2421875 KB\n"]}]},{"cell_type":"code","metadata":{"id":"lejArGUK30t9"},"source":["out_file = open(\"word_to_index3.json\", \"w\")\n","json.dump(word_to_index, out_file, indent = 6)\n","  \n","out_file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EClD9k0VAL7M"},"source":["#!mkdir ./result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8kD9YLFrjxxN","executionInfo":{"status":"ok","timestamp":1638335148253,"user_tz":300,"elapsed":46227,"user":{"displayName":"Yue Kuang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1vp258jkiryvm9TTrfO-iEBUB6bO5qRD-zHYJ=s64","userId":"13068211838488884124"}},"outputId":"9c38d400-f22c-4707-aea8-e14647fcc2eb"},"source":["global_best = 0\n","local_best = {}\n","for param in params:# hops, lr, l2_lambda, epsilon, max_grad_norm\n","  curbest = 0\n","  CONFIGS['hops'] = param[0]\n","  CONFIGS['lr'] = param[1]\n","  CONFIGS['l2_lambda'] = param[2]\n","  CONFIGS['epsilon'] = param[3]\n","  CONFIGS['max_grad_norm'] = param[4]\n","\n","  import time\n","  timestamp = time.strftime(\"%b_%d_%Y_%H_%M_%S\", time.localtime())\n","  out_file = \"./logs/set{}_{}.txt\".format(CONFIGS['set_id'], timestamp)\n","  with open(out_file, 'w', encoding='utf-8') as f:\n","    for key, value in CONFIGS.items():\n","      f.write(\"{}={}\".format(key, value))\n","      f.write(\"\\n\")\n","  \n","  train_essay_contents, train_essay_scores, train_essay_ids = load_train_data(CONFIGS['set_id'])\n","  dev_essay_contents, dev_essay_scores, dev_essay_ids = load_dev_data(CONFIGS['set_id'])\n","  test_essay_contents, test_essay_ids = load_test_data(CONFIGS['set_id'])\n","  # get the length of longest essay in training set\n","  train_sent_size_list = list(map(len, [content for content in train_essay_contents]))\n","  max_sent_size = max(train_sent_size_list)\n","  mean_sent_size = int(np.mean(train_sent_size_list))\n","  print('max_score={} \\t min_score={}'.format(max_score, min_score))\n","  print('max train sentence size={} \\t mean train sentence size={}\\n'.format(max_sent_size, mean_sent_size))\n","  with open(out_file, 'a', encoding='utf-8') as f:\n","      f.write('\\n')\n","      f.write('max_score={} \\t min_score={}\\n'.format(max_score, min_score))\n","      f.write('max sentence size={} \\t mean sentence size={}\\n'.format(max_sent_size, mean_sent_size))\n","\n","  # [train_essay_size, max_sent_size]  type: list\n","  train_contents_idx = vectorize_data(train_essay_contents, word_to_index, max_sent_size)\n","  # [dev_essay_size, max_sent_size]  type: list\n","  dev_contents_idx = vectorize_data(dev_essay_contents, word_to_index, max_sent_size)\n","  # [test_essay_size, max_sent_size]  type: list\n","  test_contents_idx = vectorize_data(test_essay_contents, word_to_index, max_sent_size)\n","\n","  memory_contents = []\n","  memory_scores = []\n","  for i in score_range:\n","      for j in range(CONFIGS['num_samples']):\n","          if i in train_essay_scores:\n","              score_idx = train_essay_scores.index(i)\n","              score = train_essay_scores.pop(score_idx)  # score=i\n","              content = train_contents_idx.pop(score_idx)\n","              memory_contents.append(content)\n","              memory_scores.append(score)\n","          else:\n","              print(f\"score {i} is not in train data\")\n","  print(\"yaha\", memory_contents)\n","  np.save('./memory/memory3', memory_contents)\n","\n","  \n","  memory_size = len(memory_contents)  # actual score_range\n","  print(memory_size)\n","  print(\"here\", len(score_range))\n","  train_scores_index = list(map(lambda x: score_range.index(x), train_essay_scores))\n","\n","  # data size\n","  n_train = len(train_contents_idx)\n","  n_dev = len(dev_contents_idx)\n","  n_test = len(test_contents_idx)\n","\n","  start_list = list(range(0, n_train - CONFIGS['batch_size'], CONFIGS['batch_size']))\n","  end_list = list(range(CONFIGS['batch_size'], n_train, CONFIGS['batch_size']))\n","  batches = zip(start_list, end_list)\n","  batches = [(start, end) for start, end in batches]\n","  if end_list[len(end_list)-1] != n_train-1:\n","      batches.append((end_list[len(end_list)-1], n_train-1))\n","  \n","\n","# model\n","  model = MANM(word_to_vec=word_to_vec, max_sent_size=max_sent_size, memory_num=memory_size, embedding_size=CONFIGS['emb_size'],\n","              feature_size=CONFIGS['feature_size'], score_range=len(score_range), hops=CONFIGS['hops'],\n","              l2_lambda=CONFIGS['l2_lambda'], keep_prob=CONFIGS['keep_prob'], device=device).to(device)\n","\n","  optimizer = torch.optim.Adam(model.parameters(), lr=CONFIGS['lr'], eps=CONFIGS['epsilon'])\n","  scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n","\n","\n","  print(\"----------begin training----------\")\n","  t1 = time.time()\n","  dev_kappa_result = 0.0\n","  for ep in range(1, CONFIGS['epochs']+1):\n","      t2 = time.time()\n","      total_loss = 0\n","      np.random.shuffle(batches)\n","      for start, end in batches:\n","          contents = np.array(train_contents_idx[start:end], dtype=np.int64)\n","          scores_index = np.array(train_scores_index[start:end], dtype=np.int64)\n","          batched_memory_contents = np.array([memory_contents]*(end-start), dtype=np.int64)\n","          optimizer.zero_grad()\n","          loss = model(contents, batched_memory_contents, scores_index)\n","          total_loss += loss.item()\n","          loss.backward()\n","          optimizer.step()\n","      t3 = time.time()\n","      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=CONFIGS['max_grad_norm'])\n","      scheduler.step(ep)\n","      print(f\"epoch {ep}/{CONFIGS['epochs']}: all loss={total_loss:.3f}, \"\n","            f\"loss/triple={(total_loss/train_essay_contents.__len__()):.6f}, \" f\"time cost={(t3-t2):.4f}\")\n","      with open(out_file, 'a', encoding='utf-8') as f:\n","          f.write(\"epoch {}: total_loss={:.3f}, loss/triple={:.6f}\\n\".format(ep, total_loss, total_loss/train_essay_contents.__len__()))\n","      # begin evaluation\n","      if ep % CONFIGS['test_freq'] == 0 or ep == CONFIGS['epochs']:\n","          print(\"------------------------------------\")\n","          mid1 = round(n_dev/3)\n","          mid2 = round(n_dev/3)*2\n","          dev_batches = [(0, mid1), (mid1, mid2), (mid2, n_dev)]\n","          all_pred_scores = []\n","          for start, end in dev_batches:\n","              dev_contents = np.array(dev_contents_idx[start:end], dtype=np.int64)\n","              batched_memory_contents = np.array([memory_contents]*dev_contents.shape[0], dtype=np.int64)\n","              pred_scores = model.test(dev_contents, batched_memory_contents).cpu().numpy()\n","              pred_scores = np.add(pred_scores, min_score)\n","              all_pred_scores += list(pred_scores)\n","          dev_kappa_result = kappa(dev_essay_scores, all_pred_scores, weights='quadratic')\n","          print(f\"kappa result={dev_kappa_result}\")\n","\n","          if dev_kappa_result > global_best:\n","            global_best = dev_kappa_result\n","            torch.save(model, \"./result/model\" + str(SET_ID) + \"_sage.pth\")\n","          curbest = max(curbest, dev_kappa_result)\n","\n","          pearson_result = pearson(dev_essay_scores, all_pred_scores)\n","          print(f\"pearson result={pearson_result}\")\n","          print(\"------------------------------------\")\n","          with open(out_file, 'a', encoding='utf-8') as f:\n","              f.write(\"------------------------------------\\n\")\n","              f.write(\"kappa result={}\\n\".format(dev_kappa_result))\n","              f.write(\"------------------------------------\\n\")\n","      if ep == CONFIGS['epochs']:\n","          print(\"----------finish training----------\\n\")\n","          print(\"\\n----------begin test----------\")\n","          print(f\"results are writen to file ./set\" + str(CONFIGS['set_id']) + \".tsv\")\n","          file_out = open(\"./result/set\" + str(CONFIGS['set_id']) + \".tsv\", 'w', encoding='utf-8')\n","          mid = round(n_test/2)\n","          test_batches = [(0, mid), (mid, n_test)]\n","          all_pred_scores = []\n","          for start, end in test_batches:\n","              test_contents = np.array(test_contents_idx[start:end], dtype=np.int64)\n","              batched_memory_contents = np.array([memory_contents]*test_contents.shape[0], dtype=np.int64)\n","              pred_scores = model.test(test_contents, batched_memory_contents).cpu().numpy()\n","              pred_scores = np.add(pred_scores, min_score)\n","              all_pred_scores += list(pred_scores)\n","          for k in range(len(test_contents_idx)):\n","              file_out.write(str(test_essay_ids[k]) + \"\\t\" + str(CONFIGS['set_id']) + \"\\t\" + str(all_pred_scores[k]) + \"\\n\")\n","          file_out.close()\n","          print(\"-----------finish test-----------\")\n","          break\n","      \n","  local_best[param] = curbest"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["max_score=125 \t min_score=5\n","max train sentence size=446 \t mean train sentence size=227\n","\n","score 5 is not in train data\n","score 6 is not in train data\n","score 7 is not in train data\n","score 8 is not in train data\n","score 9 is not in train data\n","score 11 is not in train data\n","score 12 is not in train data\n","score 13 is not in train data\n","score 14 is not in train data\n","score 16 is not in train data\n","score 17 is not in train data\n","score 18 is not in train data\n","score 19 is not in train data\n","score 20 is not in train data\n","score 21 is not in train data\n","score 22 is not in train data\n","score 23 is not in train data\n","score 24 is not in train data\n","score 25 is not in train data\n","score 26 is not in train data\n","score 27 is not in train data\n","score 28 is not in train data\n","score 29 is not in train data\n","score 31 is not in train data\n","score 32 is not in train data\n","score 33 is not in train data\n","score 34 is not in train data\n","score 36 is not in train data\n","score 38 is not in train data\n","score 39 is not in train data\n","score 40 is not in train data\n","score 41 is not in train data\n","score 42 is not in train data\n","score 43 is not in train data\n","score 44 is not in train data\n","score 46 is not in train data\n","score 47 is not in train data\n","score 48 is not in train data\n","score 49 is not in train data\n","score 51 is not in train data\n","score 52 is not in train data\n","score 53 is not in train data\n","score 54 is not in train data\n","score 55 is not in train data\n","score 57 is not in train data\n","score 58 is not in train data\n","score 61 is not in train data\n","score 63 is not in train data\n","score 64 is not in train data\n","score 66 is not in train data\n","score 67 is not in train data\n","score 68 is not in train data\n","score 69 is not in train data\n","score 74 is not in train data\n","score 76 is not in train data\n","score 79 is not in train data\n","score 82 is not in train data\n","score 83 is not in train data\n","score 86 is not in train data\n","score 88 is not in train data\n","score 89 is not in train data\n","score 90 is not in train data\n","score 91 is not in train data\n","score 92 is not in train data\n","score 94 is not in train data\n","score 95 is not in train data\n","score 97 is not in train data\n","score 98 is not in train data\n","score 99 is not in train data\n","score 101 is not in train data\n","score 102 is not in train data\n","score 103 is not in train data\n","score 104 is not in train data\n","score 105 is not in train data\n","score 106 is not in train data\n","score 107 is not in train data\n","score 108 is not in train data\n","score 109 is not in train data\n","score 110 is not in train data\n","score 111 is not in train data\n","score 112 is not in train data\n","score 113 is not in train data\n","score 114 is not in train data\n","score 115 is not in train data\n","score 116 is not in train data\n","score 117 is not in train data\n","score 118 is not in train data\n","score 119 is not in train data\n","score 121 is not in train data\n","score 122 is not in train data\n","score 123 is not in train data\n","score 124 is not in train data\n","score 125 is not in train data\n","yaha [[10, 1004, 4, 92, 2, 174, 1, 32, 25, 67, 230, 2, 348, 40, 576, 56, 534, 231, 2, 80, 239, 1, 2, 1622, 40, 576, 56, 534, 126, 4, 6, 1749, 1232, 1, 18, 243, 8, 46, 334, 10, 135, 4, 278, 18, 183, 1, 115, 43, 10, 26, 6, 1622, 10, 35, 913, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 694, 17, 15, 590, 34, 18, 348, 93, 55, 144, 1282, 32, 2, 841, 68, 46, 122, 728, 19, 18, 348, 3, 291, 4, 23, 940, 10, 120, 10, 694, 3, 1582, 7, 164, 60, 18, 348, 68, 55, 60, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [370, 1, 10, 694, 17, 12, 10, 120, 2, 926, 3, 70, 348, 434, 152, 122, 728, 4, 940, 80, 56, 194, 376, 144, 140, 22, 54, 2147, 1, 0, 1, 54, 195, 1, 3, 46, 16, 10, 120, 15, 56, 194, 29, 922, 4, 314, 54, 376, 144, 90, 174, 35, 583, 2, 1710, 4, 2, 594, 978, 30, 117, 434, 728, 4, 70, 87, 183, 7, 2, 646, 1, 3, 256, 47, 55, 144, 1, 29, 2080, 940, 80, 56, 194, 376, 144, 10, 120, 2, 348, 117, 256, 47, 34, 1202, 5, 183, 7, 18, 646, 1, 29, 2, 897, 5, 2142, 80, 80, 20, 687, 1655, 19, 2, 174, 1, 46, 10, 120, 348, 117, 888, 9, 2, 80, 1, 2, 646, 3, 2, 303, 1, 3, 314, 2, 402, 174, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 694, 926, 1, 1478, 1, 1526, 1, 3, 70, 348, 434, 152, 122, 728, 4, 2, 376, 688, 5, 940, 80, 140, 22, 250, 1230, 3, 1622, 15, 537, 6, 222, 5, 796, 9, 421, 6, 940, 1357, 3, 50, 205, 8, 2018, 2063, 1876, 115, 6, 222, 5, 348, 698, 777, 136, 1, 41, 1001, 725, 43, 30, 40, 29, 698, 16, 136, 1, 41, 72, 23, 1730, 96, 87, 46, 30, 117, 29, 698, 2, 376, 688, 5, 940, 80, 253, 1, 597, 15, 8, 334, 9, 246, 115, 0, 38, 800, 940, 80, 456, 43, 39, 40, 198, 334, 246, 93, 79, 46, 348, 117, 698, 16, 144, 546, 5, 940, 80, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [85, 5, 34, 1, 83, 2141, 2, 374, 348, 368, 56, 194, 6, 157, 5, 55, 144, 46, 12, 113, 10, 694, 17, 18, 839, 2198, 172, 307, 83, 25, 6, 374, 0, 39, 38, 533, 82, 158, 1478, 3, 695, 16, 926, 55, 0, 39, 2135, 3, 12, 6, 967, 3, 0, 113, 4, 728, 80, 4, 774, 27, 0, 17, 53, 70, 83, 2, 348, 15, 299, 80, 69, 17, 348, 82, 307, 38, 434, 9, 940, 348, 4, 1116, 80, 728, 16, 136, 3, 12, 35, 23, 368, 136, 940, 2198, 34, 2, 348, 35, 0, 12, 174, 3, 57, 4, 836, 136, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [7, 37, 251, 5, 206, 1, 10, 132, 694, 12, 1889, 111, 1202, 5, 348, 20, 434, 122, 728, 4, 2, 940, 80, 56, 194, 376, 688, 2, 348, 886, 21, 6, 152, 570, 633, 46, 12, 51, 20, 152, 122, 80, 236, 6, 1993, 732, 16, 15, 3, 15, 485, 57, 2, 250, 1230, 3, 1622, 376, 144, 20, 42, 191, 88, 2, 250, 51, 20, 6, 109, 199, 5, 992, 378, 42, 940, 4, 574, 792, 174, 80, 485, 132, 705, 7, 940, 80, 56, 194, 376, 688, 46, 2, 1658, 35, 314, 42, 174, 7, 18, 502, 140, 22, 124, 39, 239, 1, 49, 632, 1119, 39, 851, 1, 62, 0, 39, 57, 4, 126, 3, 46, 16, 15, 132, 1221, 51, 554, 144, 39, 65, 25, 64, 996, 4, 479, 54, 144, 4, 1018, 43, 30, 78, 79, 1, 99, 487, 2082, 1, 30, 132, 57, 18, 33, 2, 625, 5, 37, 8, 64, 46, 434, 122, 42, 728, 16, 49, 940, 80, 40, 87, 4, 2, 646, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 120, 1, 2, 1478, 1, 2, 367, 3, 70, 348, 1429, 1423, 47, 2, 144, 5, 940, 80, 7, 82, 864, 7, 2, 131, 2169, 1, 2139, 1930, 8, 6, 685, 539, 7, 2007, 3, 41, 8, 213, 7, 2, 174, 10, 165, 47, 136, 1188, 1, 47, 50, 699, 1, 50, 195, 1, 329, 3, 50, 1571, 90, 25, 1695, 136, 21, 6, 2193, 1095, 18, 45, 141, 136, 42, 940, 7, 2, 131, 7, 37, 869, 1, 10, 237, 2, 442, 12, 2, 348, 132, 1225, 2, 728, 5, 2, 940, 80, 163, 2, 131, 2, 348, 833, 2, 940, 80, 1034, 42, 1525, 3, 1034, 235, 152, 18, 1525, 19, 2, 348, 45, 993, 2, 886, 1760, 3, 1810, 7, 2, 578, 124, 39, 20, 663, 3, 70, 578, 7, 1411, 131, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [16, 706, 688, 358, 2199, 80, 1, 32, 98, 5, 79, 20, 2019, 44, 561, 324, 21, 37, 1633, 1365, 8, 213, 1919, 1, 32, 39, 40, 29, 531, 12, 12, 1365, 153, 128, 4, 23, 575, 115, 0, 3, 80, 63, 20, 1055, 1526, 3, 367, 521, 130, 53, 49, 10, 101, 416, 1, 39, 710, 4, 93, 235, 19, 187, 12, 96, 1866, 39, 25, 2146, 467, 39, 1531, 17, 940, 80, 115, 80, 20, 705, 7, 414, 12, 405, 5, 467, 39, 135, 4, 100, 371, 47, 1908, 7, 6, 144, 5, 90, 940, 80, 1, 115, 39, 120, 12, 16, 54, 293, 5, 2, 1566, 8, 188, 4, 144, 98, 5, 0, 2157, 0, 5, 54, 2101, 1, 3, 39, 59, 227, 79, 4, 165, 49, 39, 13, 2101, 11, 20, 2181, 7, 1018, 43, 44, 0, 1531, 467, 47, 1174, 1899, 1, 98, 5, 80, 65, 29, 118, 273, 15, 1, 3, 16, 2, 70, 528, 70, 0, 0, 523, 1244, 2052, 830, 48, 2172, 1, 63, 5, 90, 105, 0, 35, 1275, 42, 235, 33, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [30, 239, 7, 6, 131, 12, 8, 2117, 19, 348, 140, 22, 926, 1, 1526, 1, 1478, 1, 374, 348, 1, 3, 46, 16, 98, 5, 2, 467, 39, 1531, 8, 390, 17, 661, 0, 1, 9, 421, 1, 1622, 1, 999, 601, 3, 1484, 1, 10, 381, 12, 2, 251, 5, 960, 42, 47, 54, 2006, 3, 54, 1186, 8, 4, 417, 6, 2132, 203, 2, 307, 63, 8, 1380, 18, 348, 3, 2, 250, 1230, 98, 5, 2, 60, 8, 2, 116, 113, 4, 708, 198, 27, 1175, 6, 277, 1, 6, 346, 1, 27, 181, 367, 185, 10, 65, 57, 4, 227, 36, 421, 1, 43, 10, 74, 824, 6, 367, 185, 47, 37, 558, 1029, 539, 1, 3, 219, 2, 185, 39, 708, 2, 757, 41, 1733, 58, 41, 966, 1, 10, 35, 29, 120, 1133, 4, 234, 79, 102, 2, 185, 1079, 2, 367, 610, 20, 580, 9, 792, 1, 98, 5, 79, 1222, 17, 2, 863, 5, 267, 42, 80, 824, 79, 1, 43, 6, 367, 185, 8, 823, 1, 70, 1253, 3, 468, 35, 355, 2, 695, 6, 889, 5, 1892, 27, 39, 35, 434, 2, 695, 4, 23, 7, 54, 467, 7, 1460, 1, 2, 990, 5, 18, 348, 25, 6, 192, 566, 7, 1034, 42, 235, 1, 12, 8, 54, 2070, 1, 3, 15, 446, 1, 43, 80, 343, 54, 467, 39, 25, 4, 40, 198, 4, 654, 54, 2201, 3, 54, 1489, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 906, 694, 17, 12, 839, 115, 57, 39, 101, 15, 8, 54, 376, 688, 3, 2108, 1258, 117, 256, 47, 15, 225, 15, 8, 612, 3, 15, 8, 548, 12, 2, 646, 8, 96, 705, 7, 1036, 174, 5, 2, 940, 80, 1560, 15, 8, 96, 2202, 30, 27, 21, 330, 2, 1649, 1830, 7, 54, 688, 3, 12, 2109, 44, 5, 2, 443, 488, 2, 70, 1346, 8, 1, 12, 940, 80, 68, 134, 2, 1526, 1135, 6, 222, 5, 728, 1, 62, 315, 79, 42, 462, 3, 2, 581, 8, 12, 39, 72, 1275, 42, 235, 1, 62, 8, 1232, 835, 2108, 15, 8, 54, 282, 4, 23, 7, 2, 1966, 1, 32, 51, 8, 27, 117, 23, 153, 6, 1239, 203, 683, 239, 3, 501, 239, 63, 65, 272, 57, 15, 461, 6, 144, 124, 371, 14, 40, 35, 23, 788, 9, 351, 3, 1847, 0, 10, 120, 18, 8, 44, 5, 2, 878, 1, 167, 67, 1622, 1167, 6, 222, 27, 40, 1512, 39, 38, 29, 525, 17, 2, 796, 5, 2, 646, 3, 384, 842, 15, 188, 71, 39, 117, 239, 54, 2006, 46, 167, 117, 30, 29, 494, 79, 42, 455, 9, 54, 171, 3, 463, 1952, 79, 39, 20, 69, 0, 1372, 1, 63, 25, 6, 312, 1178, 62, 2032, 80, 32, 2, 1120, 243, 7, 55, 646, 8, 12, 30, 239, 9, 1940, 5, 70, 80, 30, 40, 29, 25, 2, 137, 4, 1830, 7, 6, 221, 144, 1, 513, 1972, 30, 40, 29, 100, 79, 1258, 729, 4, 25, 36, 1913, 1634, 16, 54, 501, 27, 376, 144, 803, 120, 39, 153, 38, 434, 728, 4, 1622, 1, 32, 39, 117, 463, 1140, 172, 1005, 47, 54, 144, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [748, 906, 7, 1582, 47, 960, 2, 376, 144, 5, 940, 80, 7, 367, 521, 27, 1233, 10, 132, 790, 100, 71, 1965, 107, 23, 320, 12, 243, 10, 1986, 12, 67, 5, 79, 1, 25, 82, 1124, 336, 1, 115, 84, 135, 4, 23, 2116, 27, 23, 57, 6, 265, 5, 2206, 32, 0, 39, 20, 2, 98, 940, 80, 163, 2, 131, 3, 68, 46, 122, 235, 86, 6, 496, 307, 46, 7, 252, 29, 84, 23, 2158, 4, 2, 2118, 16, 2, 70, 293, 5, 2, 367, 8, 47, 1275, 6, 222, 5, 235, 22, 39, 38, 9, 421, 1337, 1856, 1, 81, 8, 44, 5, 2, 116, 771, 1166, 307, 7, 2, 374, 436, 1, 115, 81, 250, 77, 144, 4, 1192, 469, 916, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 120, 2, 348, 434, 4, 122, 728, 4, 41, 501, 688, 5, 940, 80, 926, 1, 1478, 1, 1526, 1597, 52, 34, 1202, 5, 385, 630, 1087, 27, 59, 347, 1188, 144, 5, 221, 250, 1230, 30, 38, 221, 670, 24, 2053, 144, 5, 131, 0, 7, 221, 0, 521, 1, 719, 3, 374, 348, 384, 842, 1, 12, 1649, 5, 903, 516, 38, 68, 192, 1835, 5, 235, 9, 6, 240, 314, 27, 383, 5, 6, 0, 597, 15, 1782, 54, 872, 1, 1150, 17, 0, 27, 435, 331, 0, 1, 10, 120, 1, 12, 659, 250, 1230, 40, 29, 1713, 96, 2041, 27, 395, 790, 434, 728, 4, 221, 2030, 32, 51, 20, 67, 1143, 1, 58, 1622, 1551, 17, 0, 1622, 527, 6, 339, 96, 365, 3, 68, 6, 222, 5, 235, 15, 56, 194, 548, 402, 12, 940, 80, 104, 214, 250, 1241, 459, 365, 3, 68, 122, 235, 512, 22, 9, 61, 1, 37, 1324, 1003, 1357, 8, 1770, 10, 165, 3, 1219, 16, 367, 6, 222, 5, 1099, 133, 47, 77, 7, 1732, 5, 15, 10, 127, 77, 735, 3, 77, 2186, 96, 122, 81, 8, 1775, 1, 2175, 1, 857, 1, 892, 307, 7, 1460, 10, 266, 12, 1, 1678, 1446, 142, 63, 20, 0, 7, 940, 80, 56, 194, 376, 144, 20, 1, 7, 375, 1, 1724, 5, 54, 0, 452, 172, 0, 753, 45, 1013, 1118, 3, 49, 39, 25, 1163, 193, 1720, 1, 12, 54, 1014, 20, 771, 147, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [370, 1, 10, 694, 17, 2, 839, 489, 4, 2, 348, 1323, 89, 1525, 3, 89, 1898, 17, 2, 940, 80, 194, 688, 85, 1, 6, 482, 947, 9, 1478, 3, 1526, 7, 252, 4, 25, 6, 192, 1089, 8, 4, 1314, 53, 5, 1705, 2, 174, 9, 1229, 1, 43, 6, 1003, 515, 140, 22, 0, 26, 396, 1729, 16, 50, 550, 1, 41, 65, 956, 16, 2, 2025, 1521, 46, 1, 54, 457, 65, 23, 96, 1676, 1554, 1, 43, 2, 926, 498, 4, 1125, 161, 1226, 1, 89, 987, 117, 130, 53, 3, 372, 2, 1980, 80, 194, 1452, 15, 123, 29, 556, 43, 15, 8, 959, 27, 29, 1, 43, 15, 1911, 872, 27, 29, 1, 39, 20, 59, 698, 16, 283, 1474, 802, 1, 2, 1622, 218, 1020, 12, 39, 40, 29, 25, 501, 688, 2015, 22, 2, 348, 8, 1768, 79, 34, 2, 60, 264, 1, 102, 67, 121, 1, 2, 1230, 68, 148, 4, 267, 18, 405, 5, 1277, 7, 1075, 1, 10, 381, 12, 34, 2, 348, 1589, 6, 222, 5, 728, 4, 2, 376, 688, 5, 940, 80, 29, 84, 4, 68, 42, 616, 1, 654, 2, 457, 1, 3, 1275, 6, 192, 598, 5, 235, 32, 69, 4, 378, 42, 565, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1664, 51, 20, 111, 614, 4, 1077, 2, 174, 5, 250, 1230, 3, 1622, 140, 22, 374, 348, 1, 1478, 1, 3, 926, 512, 18, 174, 1589, 152, 122, 728, 4, 54, 376, 688, 10, 694, 17, 18, 839, 115, 172, 108, 10, 92, 122, 376, 174, 7, 374, 348, 3, 10, 40, 576, 56, 534, 266, 15, 8, 317, 27, 861, 174, 32, 2, 131, 305, 1743, 18, 328, 5, 174, 3, 315, 15, 6, 96, 1322, 9, 1143, 1, 1664, 51, 8, 6, 222, 5, 610, 1311, 7, 2, 376, 688, 5, 940, 80, 140, 22, 837, 258, 3, 15, 45, 111, 139, 3, 12, 93, 15, 6, 823, 258, 151, 15, 521, 84, 49, 90, 80, 20, 320, 172, 108, 3, 20, 39, 409, 27, 1120, 33, 69, 1, 10, 120, 30, 20, 2, 44, 63, 8, 923, 5, 18, 174, 115, 193, 2, 84, 243, 12, 521, 2, 677, 5, 82, 258, 27, 926, 185, 8, 2, 199, 5, 139, 3, 1333, 16, 18, 139, 2, 348, 8, 1309, 16, 2, 113, 12, 2, 80, 20, 1004, 67, 80, 153, 824, 131, 174, 3, 597, 39, 130, 18, 8, 96, 890, 3, 1483, 174, 264, 39, 135, 4, 1788, 2058, 46, 12, 39, 343, 18, 376, 174, 4, 59, 1315, 3, 278, 2, 1436, 51, 8, 6, 334, 293, 4, 18, 174, 513, 9, 90, 1751, 3, 39, 584, 239, 6, 782, 144, 290, 18, 174, 8, 1139, 46, 12, 15, 8, 55, 1145, 4, 220, 18, 1130, 19, 463, 824, 18, 174, 3, 291, 4, 883, 79, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [370, 10, 694, 12, 2, 926, 1, 1233, 1, 787, 1, 3, 70, 348, 434, 152, 122, 728, 4, 2, 376, 688, 5, 2, 940, 80, 115, 9, 2, 1622, 3, 940, 80, 49, 2, 256, 98, 8, 54, 752, 513, 49, 39, 135, 7, 252, 4, 93, 79, 409, 3, 29, 4, 1343, 79, 2, 311, 20, 2, 878, 12, 694, 17, 2, 839, 0, 489, 4, 1078, 54, 832, 4, 54, 752, 8, 44, 5, 98, 1274, 183, 4, 2, 1622, 2119, 940, 80, 12, 35, 149, 79, 4, 1078, 54, 847, 513, 7, 143, 761, 1766, 6, 1336, 72, 25, 2133, 652, 4, 25, 111, 143, 1074, 43, 25, 111, 752, 3, 1844, 16, 374, 1173, 15, 72, 149, 79, 4, 1078, 54, 143, 115, 98, 1622, 3, 250, 1230, 2, 698, 998, 16, 143, 1346, 46, 12, 39, 248, 23, 283, 48, 889, 5, 1748, 1, 1496, 70, 2074, 46, 12, 54, 752, 35, 237, 19, 960, 27, 848, 588, 846, 6, 196, 515, 140, 22, 6, 1003, 515, 1622, 72, 25, 6, 652, 4, 1291, 7, 6, 1129, 1148, 3, 1547, 6, 627, 143, 30, 20, 1549, 12, 102, 51, 35, 23, 6, 222, 5, 1280, 7, 332, 98, 5, 1622, 3, 250, 1230, 39, 245, 46, 122, 887, 16, 926, 1, 1233, 1, 1526, 1, 3, 70, 348, 140, 22, 374, 348, 46, 12, 39, 107, 1301, 49, 8, 1887, 21, 12, 718, 69, 39, 35, 531, 49, 80, 135, 513, 54, 752, 22, 54, 98, 1274, 815, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 694, 1, 34, 374, 348, 1, 787, 3, 1478, 8, 1134, 728, 4, 250, 753, 376, 2006, 0, 86, 317, 183, 1264, 7, 2, 131, 115, 39, 135, 4, 2179, 54, 2084, 399, 1, 80, 817, 2143, 4, 343, 2213, 753, 2006, 503, 86, 317, 3, 0, 183, 1264, 7, 1188, 144, 46, 1, 39, 20, 187, 18, 1482, 3, 1414, 54, 305, 949, 18, 1123, 4, 109, 1343, 115, 1, 39, 20, 29, 1073, 5, 131, 518, 134, 39, 25, 64, 442, 47, 1090, 3, 660, 183, 1, 2123, 20, 155, 4, 1343, 42, 60, 1039, 7, 926, 5, 824, 250, 753, 376, 144, 1, 10, 266, 656, 117, 162, 88, 18, 3, 464, 87, 1166, 4, 2123, 10, 266, 89, 29, 2170, 4, 185, 54, 376, 688, 374, 348, 13, 926, 1, 680, 1, 1794, 1135, 11, 3, 1478, 1, 1526, 39, 20, 2010, 54, 1474, 19, 817, 54, 688, 503, 86, 317, 183, 3, 1090, 10, 2174, 29, 4, 533, 2, 348, 63, 8, 906, 1765, 4, 250, 1230, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 133, 972, 348, 966, 6, 1407, 678, 7, 44, 56, 194, 144, 2, 1478, 3, 1526, 78, 96, 565, 145, 105, 1351, 9, 283, 174, 3, 70, 133, 32, 102, 2, 1673, 1959, 1, 2, 348, 9, 283, 174, 45, 812, 2, 926, 1, 643, 3, 2, 281, 45, 1330, 2, 1478, 3, 1526, 264, 2, 698, 5, 82, 174, 348, 1113, 16, 2, 250, 1230, 3, 940, 1622, 59, 115, 39, 20, 940, 3, 2, 131, 842, 79, 1, 2, 174, 348, 1894, 42, 174, 47, 79, 4, 654, 54, 910, 481, 597, 1, 2, 174, 348, 69, 1830, 1336, 56, 194, 376, 144, 4, 68, 2, 133, 1, 197, 5, 1702, 2, 174, 348, 954, 54, 1072, 9, 2, 1266, 802, 1, 39, 227, 42, 728, 4, 2, 174, 62, 20, 2, 728, 1914, 4, 2, 250, 802, 1, 4, 68, 140, 670, 1, 39, 1321, 2, 1114, 1385, 3, 326, 79, 16, 54, 1286, 1263, 39, 104, 2, 940, 1285, 9, 54, 171, 707, 39, 118, 1367, 52, 17, 2, 1622, 9, 283, 1114, 1385, 9, 421, 1, 30, 38, 92, 111, 1702, 1526, 1503, 7, 2, 2038, 17, 2, 197, 189, 383, 5, 1622, 19, 320, 18, 1, 39, 291, 4, 708, 54, 1526, 67, 1478, 69, 434, 42, 728, 4, 2, 940, 250, 1230, 146, 6, 174, 843, 1, 39, 117, 698, 16, 2, 1354, 467, 84, 1, 32, 39, 291, 4, 292, 54, 1366, 19, 784, 2, 1702, 1381, 22, 316, 37, 869, 1, 82, 174, 843, 27, 2, 673, 348, 117, 1051, 1860, 39, 20, 2, 662, 5, 2, 561, 80, 39, 117, 114, 1719, 210, 82, 988, 27, 2064, 2, 117, 191, 5, 2, 235, 1813, 1840, 3, 304, 2, 1354, 174, 84, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [172, 108, 5, 55, 144, 30, 68, 6, 222, 5, 133, 24, 926, 1, 1478, 1, 1526, 3, 70, 348, 15, 38, 23, 174, 47, 2037, 307, 1, 712, 1177, 3, 1, 5, 287, 1, 1622, 15, 91, 1963, 12, 348, 434, 4, 122, 728, 4, 2, 376, 688, 5, 940, 80, 10, 694, 17, 839, 12, 926, 135, 4, 100, 6, 222, 47, 376, 144, 1664, 30, 100, 42, 47, 250, 1230, 86, 30, 148, 260, 121, 208, 384, 38, 104, 374, 348, 57, 680, 27, 1794, 2, 786, 5, 98, 5, 90, 348, 938, 8, 4, 1077, 2, 925, 27, 1336, 320, 27, 638, 198, 334, 47, 1592, 27, 144, 454, 9, 421, 1, 30, 100, 12, 7, 391, 1885, 12, 1824, 1862, 765, 9, 1475, 24, 1542, 1808, 102, 268, 121, 5, 146, 349, 51, 78, 111, 1663, 47, 18, 395, 7, 2, 144, 5, 2, 758, 384, 498, 4, 100, 2, 392, 7, 37, 869, 2, 437, 863, 4, 1478, 3, 1526, 8, 4, 1275, 235, 1658, 40, 576, 56, 534, 25, 82, 1390, 7, 1430, 4, 279, 42, 235, 1, 210, 642, 5, 2, 1486, 9, 2, 307, 1, 32, 69, 50, 700, 1, 50, 195, 27, 1085, 50, 200, 51, 78, 111, 713, 58, 940, 80, 1826, 2, 903, 516, 1989, 1, 18, 8, 87, 749, 1, 115, 172, 44, 135, 4, 25, 67, 1452, 3, 40, 576, 56, 534, 57, 372, 376, 144, 17, 351, 4, 1757, 1, 10, 694, 12, 926, 1, 1478, 1, 1526, 1, 3, 70, 348, 434, 152, 122, 728, 4, 2, 376, 688, 5, 940, 80, 140, 22, 250, 1230, 3, 1622, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [193, 1, 30, 239, 7, 131, 197, 5, 174, 24, 940, 376, 239, 16, 926, 1, 1478, 1, 1526, 1, 3, 70, 348, 30, 38, 130, 6, 222, 5, 363, 47, 54, 942, 1, 48, 0, 3, 1150, 4, 826, 17, 1, 10, 120, 12, 30, 117, 434, 42, 728, 16, 70, 317, 499, 10, 100, 12, 9, 1348, 48, 770, 24, 1896, 2097, 38, 23, 2, 98, 317, 120, 7, 239, 1, 32, 1169, 120, 47, 55, 1152, 30, 93, 48, 1825, 1, 953, 0, 1787, 40, 29, 2086, 1839, 7, 648, 4, 12, 1, 30, 25, 0, 0, 3, 617, 2100, 1, 62, 8, 821, 1061, 9, 706, 0, 1, 250, 1230, 3, 0, 20, 1372, 152, 39, 135, 4, 25, 501, 239, 32, 365, 15, 8, 1256, 9, 79, 39, 597, 38, 29, 118, 266, 441, 7, 54, 621, 105, 121, 208, 10, 629, 47, 67, 1312, 1, 62, 1807, 7, 2, 793, 559, 1336, 94, 3, 26, 854, 9, 67, 2105, 4, 138, 67, 509, 15, 8, 40, 29, 782, 3, 10, 120, 12, 80, 117, 23, 0, 9, 140, 183, 1, 24, 2, 70, 369, 1, 348, 434, 46, 122, 728, 4, 2, 376, 688, 5, 250, 1230, 2066, 80, 135, 15, 98, 5, 91, 20, 29, 294, 4, 126, 9, 1047, 942, 27, 239, 7, 140, 6, 2079, 1023, 30, 20, 705, 7, 1936, 688, 46, 348, 875, 91, 4, 40, 15, 3, 1302, 6, 222, 5, 235, 16, 54, 282, 458, 0, 88, 1439, 1, 10, 120, 12, 348, 434, 152, 122, 728, 4, 2, 376, 688, 5, 940, 80, 39, 117, 361, 91, 42, 47, 317, 499, 3, 875, 1622, 4, 1181, 957, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [193, 56, 194, 131, 8, 6, 131, 5, 792, 30, 34, 135, 792, 7, 55, 688, 4, 68, 1393, 5, 1925, 348, 45, 378, 6, 605, 157, 5, 34, 55, 688, 58, 12, 792, 404, 7, 356, 5, 2030, 5, 55, 940, 1622, 10, 694, 17, 2, 839, 12, 926, 1, 1478, 1, 1526, 3, 70, 348, 434, 152, 122, 728, 4, 2, 376, 688, 5, 940, 80, 140, 22, 250, 1230, 3, 1622, 85, 508, 167, 348, 1962, 16, 1622, 8, 12, 39, 185, 49, 2, 250, 1508, 2, 250, 27, 1168, 20, 96, 1641, 7, 1160, 47, 49, 8, 155, 16, 7, 2, 688, 5, 54, 940, 1622, 2, 1089, 127, 4, 620, 174, 47, 54, 940, 1622, 9, 421, 1, 43, 10, 26, 414, 2, 1233, 3, 43, 51, 26, 6, 174, 47, 37, 558, 1336, 10, 65, 918, 1471, 4, 12, 519, 3, 165, 2, 174, 573, 405, 5, 1564, 593, 16, 17, 70, 80, 22, 103, 253, 508, 8, 12, 43, 39, 1531, 174, 47, 1622, 1, 2, 457, 5, 2, 200, 35, 126, 161, 18, 833, 2, 200, 1760, 22, 29, 84, 2, 457, 126, 161, 1, 54, 926, 730, 251, 593, 161, 22, 103, 39, 20, 294, 4, 708, 42, 5, 54, 0, 19, 567, 1168, 1508, 4, 1757, 1, 10, 65, 57, 4, 175, 12, 1272, 174, 47, 1622, 1107, 865, 4, 177, 2, 1168, 22, 103, 22, 2, 348, 2, 1168, 68, 2, 174, 39, 135, 3, 2, 348, 315, 2, 235, 39, 135, 10, 69, 694, 12, 348, 117, 245, 6, 1084, 7, 2160, 90, 174, 2011, 39, 117, 537, 53, 174, 62, 808, 2, 131, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2176, 1, 6, 222, 5, 2137, 191, 24, 250, 233, 513, 940, 80, 63, 213, 185, 53, 7, 348, 18, 8, 69, 847, 997, 16, 15, 22, 61, 1, 10, 1450, 694, 12, 348, 45, 185, 53, 152, 122, 376, 688, 5, 940, 80, 105, 878, 20, 80, 57, 4, 343, 15, 3, 34, 5, 348, 200, 25, 576, 56, 534, 571, 4, 0, 70, 174, 1821, 1, 2, 2137, 191, 53, 4, 80, 63, 96, 612, 9, 1229, 1, 80, 57, 4, 100, 42, 47, 2, 2197, 5, 1009, 80, 39, 1453, 47, 34, 2, 376, 144, 3, 571, 2, 113, 4, 100, 15, 16, 2, 70, 369, 1, 0, 80, 69, 25, 111, 971, 12, 80, 65, 57, 4, 408, 24, 79, 30, 100, 24, 1007, 4, 0, 1, 80, 236, 728, 7, 221, 183, 4, 408, 3, 291, 16, 15, 30, 202, 576, 56, 534, 463, 82, 612, 555, 12, 80, 117, 25, 7, 144, 64, 334, 32, 8, 402, 18, 8, 44, 5, 2, 508, 12, 348, 93, 728, 7, 940, 80, 2, 253, 508, 16, 18, 1041, 8, 2, 348, 45, 64, 2112, 3, 25, 4, 571, 15, 44, 243, 8, 2, 847, 417, 243, 138, 80, 997, 16, 15, 374, 348, 8, 157, 5, 80, 144, 115, 817, 53, 2, 376, 688, 5, 940, 80, 107, 93, 42, 235, 7, 2, 200, 1, 12, 8, 167, 39, 40, 576, 56, 534, 25, 70, 2112, 43, 39, 571, 4, 0, 53, 198, 12, 80, 40, 576, 56, 534, 612, 1, 39, 202, 576, 56, 534, 236, 1517, 4, 68, 42, 865, 1, 18, 8, 181, 508, 12, 348, 434, 728, 7, 317, 80, 7, 1460, 1, 80, 612, 4, 100, 2, 376, 688, 5, 940, 80, 3, 348, 62, 45, 64, 608, 279, 152, 122, 728, 16, 15, 30, 100, 12, 80, 1453, 6, 222, 5, 183, 1, 30, 374, 144, 106, 576, 56, 534, 820, 15, 1, 2, 728, 585, 576, 56, 534, 995, 18, 8, 69, 30, 128, 4, 120, 47, 15, 7, 55, 144, 152, 122, 728, 132, 29, 87, 4, 80, 56, 194, 144, 3, 30, 117, 299, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [7, 37, 869, 1, 10, 120, 12, 926, 1, 1478, 3, 1526, 20, 87, 21, 29, 1134, 152, 122, 728, 4, 2, 376, 688, 5, 250, 1230, 27, 1622, 503, 86, 131, 466, 10, 1479, 92, 133, 47, 1622, 58, 824, 2, 174, 182, 15, 8, 1319, 1231, 16, 131, 466, 3, 317, 1197, 12, 698, 16, 384, 1149, 1478, 20, 69, 96, 87, 21, 29, 557, 152, 122, 47, 1622, 3, 698, 16, 317, 1605, 3, 466, 7, 332, 1, 2, 80, 63, 343, 2, 174, 3, 165, 1478, 20, 547, 876, 3, 40, 29, 256, 47, 2, 144, 5, 1622, 3, 84, 47, 473, 466, 264, 1, 67, 1526, 72, 698, 6, 222, 16, 1622, 1, 32, 12, 1333, 16, 2, 896, 3, 49, 226, 12, 661, 787, 8, 1816, 4, 852, 174, 47, 2, 376, 688, 5, 1622, 3, 250, 1230, 38, 1319, 23, 207, 16, 374, 348, 124, 15, 1522, 2, 1297, 1007, 1, 63, 72, 23, 705, 7, 6, 774, 57, 12, 9, 421, 1, 43, 14, 273, 1794, 27, 680, 14, 35, 0, 130, 6, 222, 5, 665, 904, 1622, 1, 425, 15, 8, 341, 63, 45, 1650, 79, 27, 43, 15, 8, 6, 1797, 12, 8, 155, 16, 10, 266, 57, 18, 69, 45, 6, 1099, 686, 16, 2, 1297, 1007, 63, 123, 29, 165, 2, 174, 27, 343, 15, 1, 182, 39, 72, 23, 1833, 5, 473, 466, 12, 20, 1264, 7, 2, 131, 62, 107, 23, 1680, 79, 1, 210, 79, 1160, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 694, 12, 2, 348, 1589, 152, 122, 728, 4, 2, 376, 688, 5, 940, 80, 9, 421, 1, 172, 60, 58, 10, 126, 4, 6, 1562, 449, 10, 92, 142, 1526, 17, 6, 523, 1, 1337, 1856, 3, 1703, 388, 25, 6, 48, 470, 27, 1736, 1949, 8, 283, 1841, 51, 20, 1449, 5, 80, 63, 261, 1636, 54, 688, 17, 2, 688, 5, 2195, 18, 133, 38, 378, 6, 860, 9, 1345, 3, 118, 1512, 550, 230, 6, 119, 30, 620, 670, 47, 1710, 63, 1558, 1512, 115, 39, 40, 29, 162, 22, 22, 87, 22, 2, 2195, 16, 367, 3, 1526, 7, 648, 1, 67, 80, 381, 12, 2195, 3, 667, 1230, 25, 688, 12, 669, 4, 23, 137, 118, 275, 1, 111, 5, 79, 1608, 16, 54, 1890, 3, 227, 52, 16, 1444, 54, 171, 246, 80, 672, 6, 1349, 431, 7, 54, 1235, 5, 71, 54, 688, 117, 23, 325, 16, 2, 133, 39, 624, 24, 2, 348, 348, 315, 79, 952, 47, 930, 69, 1, 2, 614, 12, 940, 80, 239, 20, 29, 213, 1257, 4, 2, 1092, 3, 1089, 7, 332, 940, 80, 65, 29, 547, 227, 769, 16, 71, 4, 434, 147, 2, 2098, 27, 506, 53, 5, 867, 7, 375, 1, 98, 5, 2, 60, 39, 1167, 402, 133, 7, 1460, 1, 10, 381, 12, 80, 117, 261, 1984, 54, 688, 3, 63, 39, 20, 115, 30, 34, 239, 84, 44, 144, 1, 46, 30, 25, 4, 479, 15, 43, 51, 20, 67, 1209, 7, 55, 239, 12, 30, 634, 4, 1032, 1, 30, 218, 40, 15, 31, 264, 1, 30, 218, 190, 0, 47, 55, 688, 3, 944, 15, 17, 940, 80, 56, 194, 688, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [51, 20, 111, 930, 521, 16, 367, 12, 321, 419, 250, 1230, 56, 27, 1622, 56, 688, 39, 669, 132, 612, 9, 1617, 115, 15, 8, 2, 84, 652, 3, 113, 4, 162, 88, 71, 142, 940, 80, 239, 264, 1, 10, 120, 15, 45, 762, 152, 335, 3, 10, 40, 576, 56, 534, 120, 15, 8, 6, 87, 442, 4, 434, 152, 122, 728, 4, 54, 376, 688, 187, 661, 878, 1, 10, 35, 1094, 167, 10, 381, 46, 2, 85, 508, 10, 381, 1134, 152, 122, 728, 4, 54, 376, 688, 8, 152, 122, 8, 12, 142, 930, 521, 584, 23, 209, 1214, 17, 2, 674, 311, 142, 940, 80, 1082, 107, 93, 79, 1900, 3, 583, 79, 4, 185, 198, 221, 24, 63, 39, 132, 20, 142, 940, 80, 20, 132, 1850, 5, 71, 54, 752, 162, 21, 79, 3, 12, 8, 167, 39, 213, 291, 4, 185, 2, 87, 157, 5, 79, 803, 1, 10, 381, 142, 930, 521, 20, 29, 209, 1214, 181, 508, 10, 381, 348, 434, 152, 122, 728, 4, 2, 376, 688, 5, 940, 80, 8, 12, 15, 107, 23, 6, 1618, 5, 443, 488, 34, 443, 1626, 25, 137, 4, 23, 110, 3, 142, 940, 80, 20, 69, 443, 1626, 264, 1, 90, 348, 311, 2, 940, 80, 40, 29, 227, 79, 82, 455, 27, 720, 1018, 12, 51, 8, 341, 1929, 14, 34, 2, 60, 14, 248, 266, 2089, 142, 930, 521, 311, 1622, 172, 60, 107, 590, 6, 222, 5, 1091, 9, 2, 940, 80, 1805, 1, 39, 72, 29, 23, 87, 9, 246, 39, 38, 687, 23, 1655, 19, 49, 39, 92, 16, 367, 3, 12, 8, 167, 656, 40, 29, 135, 54, 246, 4, 343, 198, 1722, 27, 1796, 71, 47, 28, 246, 283, 1655, 3, 1270, 57, 142, 1622, 10, 74, 225, 14, 65, 29, 135, 12, 59, 57, 70, 656, 1, 14, 135, 28, 246, 4, 343, 198, 898, 1, 1110, 27, 198, 29, 46, 1545, 246, 408, 6, 222, 3, 1234, 4, 533, 49, 39, 343, 16, 367, 7, 1460, 1, 142, 930, 367, 521, 27, 70, 348, 311, 142, 1622, 107, 590, 42, 499, 140, 22, 1091, 27, 334, 1166, 3, 39, 107, 118, 1851, 443, 488, 12, 8, 167, 10, 381, 10, 694, 12, 926, 1, 1478, 1, 1526, 1, 3, 70, 348, 434, 152, 122, 728, 4, 2, 376, 688, 5, 940, 80, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 694, 12, 2, 348, 7, 332, 1589, 152, 122, 728, 4, 2, 376, 688, 5, 940, 80, 1622, 20, 146, 2110, 3, 2048, 55, 646, 8, 1861, 42, 3, 42, 501, 392, 16, 940, 1603, 1, 54, 688, 1070, 6, 930, 185, 18, 1740, 1212, 111, 1099, 1696, 66, 1156, 5, 48, 1888, 489, 4, 55, 161, 1289, 4, 2025, 1, 55, 646, 1668, 4, 2028, 2, 2005, 3, 2165, 12, 67, 1622, 690, 22, 157, 5, 54, 1871, 1, 880, 1922, 924, 18, 38, 2016, 134, 55, 1277, 1, 780, 1176, 1, 1150, 1, 1956, 1135, 80, 1234, 4, 944, 693, 4, 90, 1622, 1, 3, 452, 92, 79, 22, 678, 809, 7, 67, 713, 62, 107, 23, 1927, 4, 55, 646, 3, 424, 1563, 76, 39, 20, 1372, 152, 15, 8, 1895, 4, 1493, 2, 1882, 5, 1961, 163, 1622, 15, 8, 132, 288, 4, 1018, 49, 15, 8, 57, 4, 25, 1575, 6, 1164, 5, 1649, 1397, 28, 172, 377, 514, 18, 1579, 3, 1304, 1050, 7, 6, 637, 1, 45, 1396, 1578, 7, 111, 2120, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 723, 694, 12, 926, 1, 1478, 1, 1526, 3, 70, 348, 434, 152, 122, 728, 16, 2, 376, 688, 5, 940, 80, 7, 37, 869, 39, 40, 46, 115, 1617, 57, 4, 100, 42, 16, 2, 376, 411, 5, 2, 250, 1230, 13, 22, 39, 57, 4, 1026, 15, 27, 800, 15, 1, 15, 759, 4, 697, 52, 1681, 7, 98, 1364, 3, 604, 15, 306, 7, 921, 1525, 940, 1801, 25, 1136, 12, 572, 115, 5, 54, 722, 3, 1377, 2001, 348, 938, 117, 68, 84, 142, 379, 7, 1966, 711, 36, 421, 5, 6, 346, 515, 1699, 1, 2, 348, 117, 1026, 47, 48, 623, 12, 2, 1305, 8, 1764, 27, 1295, 71, 41, 45, 658, 1657, 1481, 7, 221, 623, 264, 1, 7, 648, 1, 2, 348, 1668, 4, 925, 50, 376, 56, 1384, 144, 56, 15, 8, 6, 96, 565, 640, 12, 56, 44, 117, 213, 236, 2, 496, 144, 1043, 24, 2, 376, 144, 56, 18, 8, 744, 49, 8, 811, 24, 348, 938, 22, 103, 15, 8, 159, 911, 608, 3, 137, 4, 236, 50, 77, 376, 144, 1912, 24, 2, 131, 3, 384, 117, 928, 12, 10, 69, 266, 12, 348, 8, 29, 1623, 923, 9, 18, 32, 1617, 20, 1472, 923, 152, 10, 25, 12, 869, 115, 348, 8, 6, 143, 3, 39, 65, 40, 49, 35, 149, 79, 7, 828, 54, 143, 384, 35, 694, 17, 61, 12, 82, 376, 1681, 16, 6, 940, 307, 35, 68, 42, 206, 1000, 22, 961, 4, 6, 484, 174, 1694, 12, 2, 164, 307, 624, 6, 1721, 814, 802, 4, 1939, 1, 370, 348, 434, 1679, 728, 7, 376, 144, 5, 940, 80, 32, 30, 1, 2, 1617, 583, 79, 4, 40, 46, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 694, 12, 1622, 956, 42, 365, 86, 39, 117, 7, 174, 1, 1526, 3, 70, 724, 5, 2067, 83, 204, 80, 669, 4, 256, 152, 122, 47, 1112, 1, 146, 879, 1, 565, 3, 1191, 737, 3, 182, 1622, 40, 34, 90, 1, 15, 8, 36, 298, 113, 5, 910, 174, 3, 1526, 3, 279, 235, 53, 5, 70, 80, 56, 194, 239, 43, 14, 944, 367, 521, 24, 193, 3, 228, 27, 276, 121, 208, 1, 14, 38, 92, 12, 30, 25, 6, 222, 5, 930, 521, 12, 40, 393, 32, 4, 533, 1622, 3, 185, 71, 39, 239, 1, 71, 39, 447, 1, 806, 54, 235, 3, 40, 962, 183, 10, 56, 526, 1147, 263, 704, 79, 152, 122, 1755, 115, 6, 222, 5, 230, 1, 940, 1603, 20, 334, 1143, 4, 939, 3, 1996, 54, 1852, 19, 910, 79, 594, 649, 47, 71, 235, 3, 229, 476, 34, 7, 144, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 40, 694, 17, 2, 442, 12, 926, 1, 1478, 1, 1526, 1, 3, 70, 348, 434, 152, 122, 728, 4, 2, 376, 688, 5, 940, 80, 10, 1319, 694, 17, 18, 839, 115, 10, 74, 294, 4, 92, 2, 1934, 5, 467, 16, 2, 348, 12, 1811, 2, 897, 5, 1622, 3, 1761, 79, 4, 2, 250, 22, 43, 15, 78, 2, 348, 56, 194, 1973, 572, 4, 40, 46, 1622, 3, 250, 1230, 20, 911, 12, 1487, 2, 137, 5, 928, 3, 897, 1, 59, 57, 82, 70, 443, 146, 16, 18, 1152, 58, 2, 348, 1921, 2, 376, 688, 5, 90, 80, 4, 2, 1728, 1, 15, 1499, 2, 250, 4, 1660, 1897, 2, 445, 3, 1087, 5, 1622, 1, 1854, 12, 39, 20, 1279, 1831, 1476, 16, 341, 39, 56, 1606, 190, 850, 512, 10, 40, 576, 56, 534, 120, 12, 2, 310, 117, 1829, 110, 1104, 5, 2, 348, 1, 10, 40, 381, 12, 39, 117, 180, 1273, 12, 1904, 2, 897, 5, 1622, 24, 146, 1426, 10, 120, 12, 58, 30, 952, 4, 1106, 1622, 3, 250, 1230, 17, 54, 1662, 488, 1, 30, 1386, 2, 734, 1718, 5, 1685, 3, 443, 137, 12, 717, 9, 34, 443, 1626, 19, 1593, 384, 1472, 1, 1299, 5, 54, 374, 1227, 1, 10, 381, 12, 30, 1429, 4, 880, 6, 131, 124, 80, 38, 853, 826, 4, 531, 159, 70, 56, 194, 1712, 3, 239, 7, 1827, 17, 159, 70, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n","28\n","here 121\n","----------begin training----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["epoch 1/200: all loss=2020.785, loss/triple=18.205269, time cost=0.3010\n","epoch 2/200: all loss=1152.722, loss/triple=10.384887, time cost=0.2643\n","epoch 3/200: all loss=923.392, loss/triple=8.318847, time cost=0.2497\n","epoch 4/200: all loss=675.119, loss/triple=6.082154, time cost=0.2386\n","epoch 5/200: all loss=653.684, loss/triple=5.889046, time cost=0.2328\n","epoch 6/200: all loss=558.530, loss/triple=5.031801, time cost=0.2237\n","epoch 7/200: all loss=507.670, loss/triple=4.573601, time cost=0.2416\n","epoch 8/200: all loss=456.584, loss/triple=4.113366, time cost=0.2216\n","epoch 9/200: all loss=470.858, loss/triple=4.241962, time cost=0.2156\n","epoch 10/200: all loss=431.443, loss/triple=3.886878, time cost=0.2160\n","epoch 11/200: all loss=442.118, loss/triple=3.983043, time cost=0.2211\n","epoch 12/200: all loss=460.983, loss/triple=4.153001, time cost=0.2147\n","epoch 13/200: all loss=446.322, loss/triple=4.020917, time cost=0.2124\n","epoch 14/200: all loss=405.372, loss/triple=3.652004, time cost=0.2131\n","epoch 15/200: all loss=394.941, loss/triple=3.558031, time cost=0.2134\n","epoch 16/200: all loss=405.197, loss/triple=3.650423, time cost=0.2164\n","epoch 17/200: all loss=374.144, loss/triple=3.370668, time cost=0.2248\n","epoch 18/200: all loss=360.618, loss/triple=3.248806, time cost=0.2141\n","epoch 19/200: all loss=344.144, loss/triple=3.100397, time cost=0.2115\n","epoch 20/200: all loss=356.090, loss/triple=3.208022, time cost=0.2162\n","------------------------------------\n","kappa result=0.0640148124259794\n","pearson result=0.06751521792824629\n","------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["epoch 21/200: all loss=280.507, loss/triple=2.527091, time cost=0.2177\n","epoch 22/200: all loss=276.302, loss/triple=2.489211, time cost=0.2173\n","epoch 23/200: all loss=273.934, loss/triple=2.467878, time cost=0.2116\n","epoch 24/200: all loss=271.510, loss/triple=2.446040, time cost=0.2136\n","epoch 25/200: all loss=266.011, loss/triple=2.396491, time cost=0.2157\n","epoch 26/200: all loss=264.163, loss/triple=2.379843, time cost=0.2141\n","epoch 27/200: all loss=261.951, loss/triple=2.359915, time cost=0.2193\n","epoch 28/200: all loss=259.275, loss/triple=2.335813, time cost=0.2161\n","epoch 29/200: all loss=258.453, loss/triple=2.328403, time cost=0.2275\n","epoch 30/200: all loss=257.604, loss/triple=2.320753, time cost=0.2207\n","epoch 31/200: all loss=255.057, loss/triple=2.297812, time cost=0.2132\n","epoch 32/200: all loss=254.276, loss/triple=2.290776, time cost=0.2133\n","epoch 33/200: all loss=253.820, loss/triple=2.286665, time cost=0.2099\n","epoch 34/200: all loss=250.551, loss/triple=2.257214, time cost=0.2188\n","epoch 35/200: all loss=251.721, loss/triple=2.267753, time cost=0.2123\n","epoch 36/200: all loss=249.223, loss/triple=2.245254, time cost=0.2110\n","epoch 37/200: all loss=248.677, loss/triple=2.240337, time cost=0.2199\n","epoch 38/200: all loss=248.245, loss/triple=2.236438, time cost=0.2213\n","epoch 39/200: all loss=246.803, loss/triple=2.223450, time cost=0.2126\n","epoch 40/200: all loss=246.334, loss/triple=2.219223, time cost=0.2108\n","------------------------------------\n","kappa result=0.12306424009684014\n","pearson result=0.12590246972465402\n","------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["epoch 41/200: all loss=245.741, loss/triple=2.213886, time cost=0.2146\n","epoch 42/200: all loss=245.173, loss/triple=2.208768, time cost=0.2184\n","epoch 43/200: all loss=245.480, loss/triple=2.211532, time cost=0.2152\n","epoch 44/200: all loss=244.673, loss/triple=2.204265, time cost=0.2125\n","epoch 45/200: all loss=244.225, loss/triple=2.200222, time cost=0.2146\n","epoch 46/200: all loss=243.494, loss/triple=2.193643, time cost=0.2119\n","epoch 47/200: all loss=243.647, loss/triple=2.195015, time cost=0.2148\n","epoch 48/200: all loss=243.400, loss/triple=2.192793, time cost=0.2140\n","epoch 49/200: all loss=242.812, loss/triple=2.187494, time cost=0.2122\n","epoch 50/200: all loss=242.682, loss/triple=2.186326, time cost=0.2162\n","epoch 51/200: all loss=242.544, loss/triple=2.185081, time cost=0.2116\n","epoch 52/200: all loss=242.221, loss/triple=2.182169, time cost=0.2142\n","epoch 53/200: all loss=241.708, loss/triple=2.177547, time cost=0.2150\n","epoch 54/200: all loss=242.472, loss/triple=2.184431, time cost=0.2152\n","epoch 55/200: all loss=241.589, loss/triple=2.176475, time cost=0.2173\n","epoch 56/200: all loss=241.982, loss/triple=2.180014, time cost=0.2302\n","epoch 57/200: all loss=241.544, loss/triple=2.176068, time cost=0.2163\n","epoch 58/200: all loss=241.271, loss/triple=2.173617, time cost=0.2141\n","epoch 59/200: all loss=240.974, loss/triple=2.170937, time cost=0.2155\n","epoch 60/200: all loss=240.960, loss/triple=2.170812, time cost=0.2183\n","------------------------------------\n","kappa result=-0.1547039789947342\n","pearson result=-0.15901895968638752\n","------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["epoch 61/200: all loss=240.964, loss/triple=2.170848, time cost=0.2183\n","epoch 62/200: all loss=240.634, loss/triple=2.167871, time cost=0.2274\n","epoch 63/200: all loss=240.495, loss/triple=2.166619, time cost=0.2202\n","epoch 64/200: all loss=240.414, loss/triple=2.165894, time cost=0.2141\n","epoch 65/200: all loss=240.306, loss/triple=2.164915, time cost=0.2225\n","epoch 66/200: all loss=240.288, loss/triple=2.164756, time cost=0.2187\n","epoch 67/200: all loss=240.179, loss/triple=2.163772, time cost=0.2171\n","epoch 68/200: all loss=240.017, loss/triple=2.162315, time cost=0.2124\n","epoch 69/200: all loss=239.995, loss/triple=2.162117, time cost=0.2138\n","epoch 70/200: all loss=239.970, loss/triple=2.161893, time cost=0.2316\n","epoch 71/200: all loss=239.800, loss/triple=2.160362, time cost=0.2199\n","epoch 72/200: all loss=239.687, loss/triple=2.159340, time cost=0.2171\n","epoch 73/200: all loss=239.646, loss/triple=2.158973, time cost=0.2144\n","epoch 74/200: all loss=239.621, loss/triple=2.158746, time cost=0.2207\n","epoch 75/200: all loss=239.539, loss/triple=2.158012, time cost=0.2126\n","epoch 76/200: all loss=239.444, loss/triple=2.157151, time cost=0.2111\n","epoch 77/200: all loss=239.389, loss/triple=2.156658, time cost=0.2179\n","epoch 78/200: all loss=239.390, loss/triple=2.156668, time cost=0.2133\n","epoch 79/200: all loss=239.344, loss/triple=2.156257, time cost=0.2219\n","epoch 80/200: all loss=239.296, loss/triple=2.155822, time cost=0.2154\n","------------------------------------\n","kappa result=-0.050402867329846\n","pearson result=-0.052157055654652736\n","------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["epoch 81/200: all loss=239.229, loss/triple=2.155214, time cost=0.2182\n","epoch 82/200: all loss=239.149, loss/triple=2.154499, time cost=0.2133\n","epoch 83/200: all loss=239.137, loss/triple=2.154390, time cost=0.2198\n","epoch 84/200: all loss=239.057, loss/triple=2.153668, time cost=0.2189\n","epoch 85/200: all loss=239.025, loss/triple=2.153377, time cost=0.2207\n","epoch 86/200: all loss=239.055, loss/triple=2.153651, time cost=0.2124\n","epoch 87/200: all loss=238.974, loss/triple=2.152915, time cost=0.2156\n","epoch 88/200: all loss=238.956, loss/triple=2.152761, time cost=0.2121\n","epoch 89/200: all loss=238.927, loss/triple=2.152496, time cost=0.2139\n","epoch 90/200: all loss=238.871, loss/triple=2.151993, time cost=0.2120\n","epoch 91/200: all loss=238.877, loss/triple=2.152045, time cost=0.2154\n","epoch 92/200: all loss=238.821, loss/triple=2.151544, time cost=0.2225\n","epoch 93/200: all loss=238.802, loss/triple=2.151368, time cost=0.2168\n","epoch 94/200: all loss=238.792, loss/triple=2.151283, time cost=0.2141\n","epoch 95/200: all loss=238.753, loss/triple=2.150931, time cost=0.2200\n","epoch 96/200: all loss=238.729, loss/triple=2.150711, time cost=0.2136\n","epoch 97/200: all loss=238.728, loss/triple=2.150701, time cost=0.2179\n","epoch 98/200: all loss=238.710, loss/triple=2.150543, time cost=0.2207\n","epoch 99/200: all loss=238.689, loss/triple=2.150354, time cost=0.2178\n","epoch 100/200: all loss=238.657, loss/triple=2.150066, time cost=0.2119\n","------------------------------------\n","kappa result=0.272223385381657\n","pearson result=0.27480548007874767\n","------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["epoch 101/200: all loss=238.639, loss/triple=2.149905, time cost=0.2167\n","epoch 102/200: all loss=238.634, loss/triple=2.149858, time cost=0.2122\n","epoch 103/200: all loss=238.626, loss/triple=2.149787, time cost=0.2162\n","epoch 104/200: all loss=238.608, loss/triple=2.149618, time cost=0.2149\n","epoch 105/200: all loss=238.595, loss/triple=2.149509, time cost=0.2083\n","epoch 106/200: all loss=238.585, loss/triple=2.149414, time cost=0.2159\n","epoch 107/200: all loss=238.580, loss/triple=2.149365, time cost=0.2102\n","epoch 108/200: all loss=238.561, loss/triple=2.149197, time cost=0.2215\n","epoch 109/200: all loss=238.544, loss/triple=2.149047, time cost=0.2147\n","epoch 110/200: all loss=238.536, loss/triple=2.148974, time cost=0.2174\n","epoch 111/200: all loss=238.520, loss/triple=2.148827, time cost=0.2133\n","epoch 112/200: all loss=238.528, loss/triple=2.148901, time cost=0.2134\n","epoch 113/200: all loss=238.503, loss/triple=2.148672, time cost=0.2096\n","epoch 114/200: all loss=238.491, loss/triple=2.148570, time cost=0.2161\n","epoch 115/200: all loss=238.486, loss/triple=2.148526, time cost=0.2151\n","epoch 116/200: all loss=238.483, loss/triple=2.148499, time cost=0.2144\n","epoch 117/200: all loss=238.478, loss/triple=2.148451, time cost=0.2097\n","epoch 118/200: all loss=238.463, loss/triple=2.148311, time cost=0.2129\n","epoch 119/200: all loss=238.459, loss/triple=2.148283, time cost=0.2123\n","epoch 120/200: all loss=238.452, loss/triple=2.148220, time cost=0.2167\n","------------------------------------\n","kappa result=0.3033005136292687\n","pearson result=0.3062824869772039\n","------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["epoch 121/200: all loss=238.450, loss/triple=2.148202, time cost=0.2106\n","epoch 122/200: all loss=238.447, loss/triple=2.148174, time cost=0.2136\n","epoch 123/200: all loss=238.434, loss/triple=2.148057, time cost=0.2152\n","epoch 124/200: all loss=238.432, loss/triple=2.148040, time cost=0.2120\n","epoch 125/200: all loss=238.428, loss/triple=2.148003, time cost=0.2125\n","epoch 126/200: all loss=238.420, loss/triple=2.147924, time cost=0.2108\n","epoch 127/200: all loss=238.419, loss/triple=2.147916, time cost=0.2094\n","epoch 128/200: all loss=238.416, loss/triple=2.147894, time cost=0.2161\n","epoch 129/200: all loss=238.406, loss/triple=2.147801, time cost=0.2145\n","epoch 130/200: all loss=238.405, loss/triple=2.147796, time cost=0.2250\n","epoch 131/200: all loss=238.401, loss/triple=2.147761, time cost=0.2135\n","epoch 132/200: all loss=238.399, loss/triple=2.147735, time cost=0.2202\n","epoch 133/200: all loss=238.393, loss/triple=2.147689, time cost=0.2174\n","epoch 134/200: all loss=238.390, loss/triple=2.147655, time cost=0.2162\n","epoch 135/200: all loss=238.386, loss/triple=2.147624, time cost=0.2214\n","epoch 136/200: all loss=238.383, loss/triple=2.147591, time cost=0.2152\n","epoch 137/200: all loss=238.379, loss/triple=2.147557, time cost=0.2208\n","epoch 138/200: all loss=238.377, loss/triple=2.147539, time cost=0.2182\n","epoch 139/200: all loss=238.379, loss/triple=2.147561, time cost=0.2167\n","epoch 140/200: all loss=238.375, loss/triple=2.147523, time cost=0.2172\n","------------------------------------\n","kappa result=0.272223385381657\n","pearson result=0.27480548007874767\n","------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["epoch 141/200: all loss=238.371, loss/triple=2.147488, time cost=0.2176\n","epoch 142/200: all loss=238.372, loss/triple=2.147499, time cost=0.2182\n","epoch 143/200: all loss=238.368, loss/triple=2.147457, time cost=0.2096\n","epoch 144/200: all loss=238.365, loss/triple=2.147429, time cost=0.2119\n","epoch 145/200: all loss=238.364, loss/triple=2.147423, time cost=0.2148\n","epoch 146/200: all loss=238.360, loss/triple=2.147391, time cost=0.2174\n","epoch 147/200: all loss=238.361, loss/triple=2.147395, time cost=0.2134\n","epoch 148/200: all loss=238.360, loss/triple=2.147389, time cost=0.2156\n","epoch 149/200: all loss=238.357, loss/triple=2.147360, time cost=0.2096\n","epoch 150/200: all loss=238.355, loss/triple=2.147345, time cost=0.2206\n","epoch 151/200: all loss=238.354, loss/triple=2.147332, time cost=0.2188\n","epoch 152/200: all loss=238.353, loss/triple=2.147327, time cost=0.2132\n","epoch 153/200: all loss=238.350, loss/triple=2.147302, time cost=0.2145\n","epoch 154/200: all loss=238.351, loss/triple=2.147303, time cost=0.2267\n","epoch 155/200: all loss=238.349, loss/triple=2.147285, time cost=0.2112\n","epoch 156/200: all loss=238.348, loss/triple=2.147281, time cost=0.2208\n","epoch 157/200: all loss=238.347, loss/triple=2.147270, time cost=0.2128\n","epoch 158/200: all loss=238.347, loss/triple=2.147266, time cost=0.2118\n","epoch 159/200: all loss=238.345, loss/triple=2.147251, time cost=0.2292\n","epoch 160/200: all loss=238.344, loss/triple=2.147245, time cost=0.2184\n","------------------------------------\n","kappa result=0.272223385381657\n","pearson result=0.27480548007874767\n","------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["epoch 161/200: all loss=238.343, loss/triple=2.147238, time cost=0.2171\n","epoch 162/200: all loss=238.342, loss/triple=2.147227, time cost=0.2120\n","epoch 163/200: all loss=238.342, loss/triple=2.147221, time cost=0.2112\n","epoch 164/200: all loss=238.341, loss/triple=2.147217, time cost=0.2122\n","epoch 165/200: all loss=238.341, loss/triple=2.147212, time cost=0.2303\n","epoch 166/200: all loss=238.339, loss/triple=2.147202, time cost=0.2088\n","epoch 167/200: all loss=238.339, loss/triple=2.147196, time cost=0.2132\n","epoch 168/200: all loss=238.338, loss/triple=2.147189, time cost=0.2106\n","epoch 169/200: all loss=238.338, loss/triple=2.147186, time cost=0.2107\n","epoch 170/200: all loss=238.337, loss/triple=2.147184, time cost=0.2126\n","epoch 171/200: all loss=238.337, loss/triple=2.147180, time cost=0.2208\n","epoch 172/200: all loss=238.337, loss/triple=2.147177, time cost=0.2120\n","epoch 173/200: all loss=238.336, loss/triple=2.147170, time cost=0.2117\n","epoch 174/200: all loss=238.335, loss/triple=2.147166, time cost=0.2150\n","epoch 175/200: all loss=238.335, loss/triple=2.147163, time cost=0.2159\n","epoch 176/200: all loss=238.335, loss/triple=2.147159, time cost=0.2117\n","epoch 177/200: all loss=238.335, loss/triple=2.147159, time cost=0.2144\n","epoch 178/200: all loss=238.334, loss/triple=2.147151, time cost=0.2165\n","epoch 179/200: all loss=238.333, loss/triple=2.147148, time cost=0.2193\n","epoch 180/200: all loss=238.333, loss/triple=2.147145, time cost=0.2141\n","------------------------------------\n","kappa result=0.272223385381657\n","pearson result=0.27480548007874767\n","------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["epoch 181/200: all loss=238.333, loss/triple=2.147144, time cost=0.2138\n","epoch 182/200: all loss=238.333, loss/triple=2.147141, time cost=0.2137\n","epoch 183/200: all loss=238.333, loss/triple=2.147140, time cost=0.2214\n","epoch 184/200: all loss=238.332, loss/triple=2.147136, time cost=0.2184\n","epoch 185/200: all loss=238.332, loss/triple=2.147134, time cost=0.2158\n","epoch 186/200: all loss=238.332, loss/triple=2.147132, time cost=0.2199\n","epoch 187/200: all loss=238.331, loss/triple=2.147130, time cost=0.2198\n","epoch 188/200: all loss=238.331, loss/triple=2.147130, time cost=0.2157\n","epoch 189/200: all loss=238.331, loss/triple=2.147128, time cost=0.2161\n","epoch 190/200: all loss=238.331, loss/triple=2.147125, time cost=0.2179\n","epoch 191/200: all loss=238.331, loss/triple=2.147126, time cost=0.2249\n","epoch 192/200: all loss=238.331, loss/triple=2.147125, time cost=0.2156\n","epoch 193/200: all loss=238.330, loss/triple=2.147122, time cost=0.2183\n","epoch 194/200: all loss=238.331, loss/triple=2.147122, time cost=0.2095\n","epoch 195/200: all loss=238.330, loss/triple=2.147119, time cost=0.2128\n","epoch 196/200: all loss=238.330, loss/triple=2.147119, time cost=0.2160\n","epoch 197/200: all loss=238.330, loss/triple=2.147117, time cost=0.2267\n","epoch 198/200: all loss=238.330, loss/triple=2.147117, time cost=0.2124\n","epoch 199/200: all loss=238.330, loss/triple=2.147116, time cost=0.2121\n","epoch 200/200: all loss=238.330, loss/triple=2.147115, time cost=0.2227\n","------------------------------------\n","kappa result=0.272223385381657\n","pearson result=0.27480548007874767\n","------------------------------------\n","----------finish training----------\n","\n","\n","----------begin test----------\n","results are writen to file ./set3.tsv\n","-----------finish test-----------\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GIp1UoHMSdCd","executionInfo":{"status":"ok","timestamp":1638335162121,"user_tz":300,"elapsed":841,"user":{"displayName":"Yue Kuang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1vp258jkiryvm9TTrfO-iEBUB6bO5qRD-zHYJ=s64","userId":"13068211838488884124"}},"outputId":"bd13852d-0422-4046-93b0-2f4b918fe7a8"},"source":["print(local_best)\n","print(global_best)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{(1, 0.0005, 0.1, 0.1, 10.0): 0.3033005136292687}\n","0.3033005136292687\n"]}]},{"cell_type":"markdown","metadata":{"id":"XUdtxGGU747t"},"source":["## Kappa Result\n","1. 0.8+, lr = 0.002 \n","2. 0.7038, lr = 0.001\n","3. 0.8113, lr = 0.0005 hops=1 l2_lambda=0.1\n","4. 0,6198, \n","\"hops\" : 1 ,#Number of hops in the Memory Network\n","\"lr\" : 0.0005,#Learning rate\n","\"batch_size\" : 16 ,#Batch size for training\n","\"l2_lambda\" : 0.0 ,#Lambda for l2 loss\n","\"epsilon\" : 0.001 ,#Epsilon value for Adam Optimizer\n","\"max_grad_norm\" : 2.0 ,#Clip gradients to this norm\n","\"keep_prob\" : 1 ,#Keep probability for dropout\n","5. 0.690\n","hops=2\n","lr=0.0005\n","batch_size=16\n","l2_lambda=0.1\n","num_samples=1\n","epsilon=0.1\n","max_grad_norm=6.0\n","keep_prob=0.95\n","6. 0.5224, lr = 0.001\n","7. 0.5375, lr = 0.001\n","8. 0.7423, lr = 0.001\n","9. 0.8286, lr = 0.001\n","10. 0.7332，lr = 0.001\n","11. 0.7997, lr = 0.001\n","12. 0.5885, lr = 0.001\n","13. 0.4237, lr = 0.001\n","14. 0.7980, lr = 0.001"]},{"cell_type":"markdown","metadata":{"id":"Tpes9Vu-JXpD"},"source":["## Save Model\n","\n","- Reference: https://pytorch.org/tutorials/beginner/saving_loading_models.html"]},{"cell_type":"code","metadata":{"id":"Y63pt1eQHEGu"},"source":["torch.save(model, \"./result/model4_new.bin\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xKST2gZq0WFj"},"source":["torch.save({\n","            'memory': memory_contents,\n","            'model_state_dict': model.state_dict(),\n","            'word_to_index': word_to_index,\n","            'config': {\n","                'embedding': model.embedding_size,\n","                'feature_size': model.feature_size,\n","                'hops': model.hops,\n","                'l2_lambda': model.l2_lambda,\n","                'keep_prob': model.keep_prob\n","            }\n","            }, './models/model3_everything.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KpSRtbEhJ4KZ"},"source":["## Load Model"]},{"cell_type":"code","metadata":{"id":"sxuAdoPnJDY8"},"source":["import torch\n","loaded_model = torch.load(\"./result/model.bin\")\n","loaded_model.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n0JQbY6BKKao"},"source":["pred_scores = loaded_model.test(dev_contents, batched_memory_contents).cpu().numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4zn6Tp-bsNaj","executionInfo":{"elapsed":417,"status":"ok","timestamp":1635489138667,"user":{"displayName":"Yue Kuang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1vp258jkiryvm9TTrfO-iEBUB6bO5qRD-zHYJ=s64","userId":"13068211838488884124"},"user_tz":240},"outputId":"37e87ff3-a274-44f6-cfc0-d80d06d24df6"},"source":["model.test(test_contents, batched_memory_contents).cpu().numpy()"],"execution_count":null,"outputs":[{"data":{"text/plain":["array([750, 750, 625, 750, 625, 625, 700, 700, 750])"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"oedOUs7P8-uE"},"source":[""],"execution_count":null,"outputs":[]}]}